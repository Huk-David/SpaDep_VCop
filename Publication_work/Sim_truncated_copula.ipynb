{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u1819911\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Load packages and functions\n",
    "%matplotlib inline\n",
    "from scipy.optimize import dual_annealing\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from collections import Counter\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.metrics.pairwise import rbf_kernel as rbf\n",
    "from math import sqrt\n",
    "from joblib import Parallel, delayed\n",
    "import psutil\n",
    "import math\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "import scipy.sparse\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import multiprocessing\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "from scipy.optimize import dual_annealing\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from collections import Counter\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "import psutil\n",
    "import math\n",
    "#import scikit.sparse as cholmod\n",
    "\n",
    "\n",
    "#from parallel.backends import BackendDummy as Backend\n",
    "#backend = Backend()\n",
    "#################\n",
    "with open('inv_Us.txt','rb') as f:\n",
    "    inv_us = pickle.load(f)\n",
    "\n",
    "with open('did_rain_01.txt','rb') as f:\n",
    "    did_rain_01 = pickle.load(f)\n",
    "\n",
    "with open('g2_us.txt','rb') as f:\n",
    "    g2_us = pickle.load(f)\n",
    "\n",
    "with open('dist_mat.txt','rb') as f:\n",
    "    dist_mat = pickle.load(f)\n",
    "\n",
    "# Functions\n",
    "\n",
    "# MC estimate - Correct and working\n",
    "\n",
    "def mc_cdf(rvs,cov_matrix,ppf_us):\n",
    "    '''\n",
    "    rvs: [[set1], ... , [set m]] each set of length = #locations = n.\n",
    "    cov_matrix: covariance matrix between the locations in question.\n",
    "    ppf_us: [u1, ... , un] length = # locs.\n",
    "    '''    \n",
    "    # rvs are mean 0 already, but need different covariance\n",
    "    cholesk_mat = np.linalg.cholesky(cov_matrix)\n",
    "    normcov_vars = [np.asarray(cholesk_mat@rvs[i]) for i in range(len(rvs))]\n",
    "    out = max(1e-13,np.sum([np.sum([normcov_vars[m][n]<ppf_us[n] for n in range(len(ppf_us))])==len(ppf_us) for m in range(len(normcov_vars))]))\n",
    "    return out/len(rvs)       \n",
    "\n",
    "# New cdf calculation - correct and working\n",
    "def mvn_cdf(a,b,Sigma):\n",
    "    m = len(a)\n",
    "    # Reorder Sigma, a and b according to the length of interval (higher earlier)\n",
    "    len_interval = np.zeros(m)\n",
    "    for ind in range(m):\n",
    "        if a[ind] == -np.inf:\n",
    "            a[ind] = -100000\n",
    "        if b[ind] == np.inf:\n",
    "            b[ind] = 100000\n",
    "        len_interval[ind] = b[ind] - a[ind]\n",
    "    order_len_interval = [int(i[0]) for i in sorted(enumerate(len_interval), key=lambda x:x[1], reverse=True)]\n",
    "    Sigma = Sigma[order_len_interval, :][:,order_len_interval]\n",
    "    a = [a[x] for x in order_len_interval]\n",
    "    b = [b[x] for x in order_len_interval]\n",
    "\n",
    "\n",
    "    d = np.zeros(shape=(m,))\n",
    "    e = np.ones(shape=(m,))\n",
    "    f = np.zeros(shape=(m,))\n",
    "    y = np.zeros(shape=(m,))\n",
    "    # these can be changed for different precision levels\n",
    "    epsilon = 1.e-5\n",
    "    alpha = 2.5\n",
    "    N_max = int(1e5)\n",
    "\n",
    "    # Compute Cholesky decomposition of Sigma to produce lower trinagualr matrix\n",
    "    C = np.linalg.cholesky(Sigma)\n",
    "\n",
    "    #Sigma_sparse = sparse.csc_matrix(Sigma)\n",
    "    #C_sparse = cholmod.cholesky(Sigma_sparse).L()\n",
    "    #C = C_sparse\n",
    "\n",
    "    Intsum, N, Varsum = 0, 0, 0\n",
    "\n",
    "    if a[0] != -np.inf:\n",
    "        d[0] = scs.norm.cdf(a[0]/C[0,0])\n",
    "    if b[0] != np.inf:\n",
    "        e[0] = scs.norm.cdf(b[0]/C[0,0])\n",
    "    f[0] = np.log(e[0] - d[0])\n",
    "\n",
    "    for ind_N in range(N_max):\n",
    "        \n",
    "        w = np.random.rand(m)\n",
    "        for ind_m in range(1,m):\n",
    "            y[ind_m-1] = scs.norm.ppf(d[ind_m-1]+w[ind_m-1]*(e[ind_m-1]-d[ind_m-1]))\n",
    "            if a[ind_m] != -np.inf:\n",
    "                d[ind_m] = scs.norm.cdf((a[ind_m]-sum(C[ind_m, :ind_m]*y[:ind_m]))/C[ind_m,ind_m])\n",
    "            if b[ind_m] != np.inf:\n",
    "                e[ind_m] = scs.norm.cdf((b[ind_m]-sum(C[ind_m, :ind_m]*y[:ind_m]))/C[ind_m,ind_m])\n",
    "            f[ind_m] = np.log((e[ind_m] - d[ind_m])) + f[ind_m-1]\n",
    "        N = N+1\n",
    "        delta = (f[-1]-Intsum)/N\n",
    "        Intsum = Intsum + delta\n",
    "        Varsum = (N-2)*Varsum/N + pow(delta, 2)\n",
    "        Error = alpha*np.sqrt(Varsum)\n",
    "        if Error < epsilon:\n",
    "            break\n",
    "\n",
    "    return(Intsum)\n",
    "# Truncated Gaussian copula class\n",
    "class truncgauss():\n",
    "    def __init__(self,L):\n",
    "        '''\n",
    "        Class to fit and work with a truncated Gaussian copula, using two functions.\n",
    "        sim: simulated [0,1] values using this copula density.\n",
    "        eval_nll: evaluate the negative log likelihood based on given data.\n",
    "        To use the class, the dimension of the data is needed.\n",
    "        n: dimension of data\n",
    "        '''\n",
    "        self.L=L\n",
    "    \n",
    "    def sim(self,theta,inv_us,draws,day_idx):\n",
    "        '''\n",
    "        Simulates m draws from the truncated gaussian copula conditional on parameters theta for the covariance kernel.\n",
    "        '''\n",
    "        # Generate mvn with Sigma, then truncate according to p_i\n",
    "        n = len(self.L)\n",
    "        cov_mat = rbf(self.L,gamma=theta)\n",
    "        rvs = scs.multivariate_normal.rvs(np.zeros(n),cov_mat,size=draws)\n",
    "        rvs_cov = [np.asarray(np.linalg.cholesky(cov_mat)@rvs[i]) for i in range(len(rvs))]\n",
    "\n",
    "        return [[ scs.norm.cdf(max([rvs_cov[i][j],inv_us[day_idx][j]])) for j in range(n)] for i in range(draws)]\n",
    "\n",
    "    def nll(self,theta,Invcdf_Us,truncation_pi,did_rain,len_locs,rvs):\n",
    "        cov_mat = np.nan_to_num(rbf(self.L,gamma=theta),copy=False,nan=0)\n",
    "        nll=0\n",
    "        for day in tqdm(range(len(Invcdf_Us))): #eg [251,...,500]\n",
    "            \n",
    "            # numerator: joint pdf integrated over truncation - equivalent to elegant normal cdf in some cases. Check cases:\n",
    "            if np.sum(did_rain[day])==len(did_rain[day]): #all wet, just normal pdf\n",
    "                nll += scs.multivariate_normal.logpdf(x=Invcdf_Us[day],mean=np.zeros(len(Invcdf_Us[0])),cov=cov_mat)\n",
    "            else: # some dry -> use elegant cdf\n",
    "                #print('elegant'+str(np.sum(did_rain[day])))\n",
    "                if np.sum(did_rain[day])==0: # all dry -> use normal cdf\n",
    "                    nll += scs.multivariate_normal.logcdf(x=Invcdf_Us[day],mean=np.zeros(len(Invcdf_Us[0])),cov=cov_mat)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    # need to re-arrange the covariance matrix into 2*2 blocks based on censored/uncensored. \n",
    "                    r_rain,c_rain = [[i] for i in np.nonzero(did_rain[day])[0]],[i for i in np.nonzero(did_rain[day])[0]]\n",
    "                    r_dry,c_dry = [[i] for i in range(len_locs) if i not in np.nonzero(did_rain[day])[0]],[i for i in range(len_locs) if i not in np.nonzero(did_rain[day])[0]]\n",
    "                    \n",
    "                    # create submatricies\n",
    "                    submat11 = cov_mat[r_dry,c_dry]\n",
    "                    submat12 = cov_mat[r_dry,c_rain]\n",
    "                    submat21 = cov_mat[r_rain,c_dry]\n",
    "                    submat22 = cov_mat[r_rain,c_rain]\n",
    "                   \n",
    "                    # compute and add the elegant cdf\n",
    "                    if len(submat22)==1:\n",
    "                        nll += scs.multivariate_normal.logcdf(x=[Invcdf_Us[day][k] for k in c_dry],mean=np.zeros(len(submat11)),cov=(submat11-submat12*np.linalg.inv(submat22)[0]*submat21))\n",
    "                    else:\n",
    "                        nll += scs.multivariate_normal.logcdf(x=[Invcdf_Us[day][k] for k in c_dry],mean=np.zeros(len(submat11)),cov=(submat11-submat12@np.linalg.inv(submat22)@submat21))\n",
    "            \n",
    "            # denominator: multiplication of marginals - pdf for positive rain, cdf for truncated\n",
    "            for loc in range(len(Invcdf_Us[0])):\n",
    "                if did_rain[day][loc]==1:# wet: add pdf(invcdf(u_i))\n",
    "                    nll -= scs.norm.logpdf(Invcdf_Us[day][loc],loc=0,scale=1)\n",
    "                else:# dry: add cdf(invcdf(pi))\n",
    "                    nll -= np.log(truncation_pi[day][loc])\n",
    "            \n",
    "        return -nll\n",
    "\n",
    "    def nll_mc(self,theta,Invcdf_Us,truncation_pi,did_rain,len_locs,rvs):\n",
    "        cov_mat = np.nan_to_num(rbf(self.L,gamma=theta),copy=False,nan=0)\n",
    "        nll=0\n",
    "        for day in tqdm(range(len(Invcdf_Us))): #eg [251,...,500]\n",
    "            \n",
    "            # numerator: joint pdf integrated over truncation - equivalent to elegant normal cdf in some cases. Check cases:\n",
    "            if np.sum(did_rain[day])==len(did_rain[day]): #all wet, just normal pdf\n",
    "                nll += scs.multivariate_normal.logpdf(x=Invcdf_Us[day],mean=np.zeros(len(Invcdf_Us[0])),cov=cov_mat)\n",
    "            else: # some dry -> use elegant cdf\n",
    "                #print('elegant'+str(np.sum(did_rain[day])))\n",
    "                if np.sum(did_rain[day])==0: # all dry -> use normal cdf\n",
    "                    nll += scs.multivariate_normal.logcdf(x=Invcdf_Us[day],mean=np.zeros(len(Invcdf_Us[0])),cov=cov_mat)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    # need to re-arrange the covariance matrix into 2*2 blocks based on censored/uncensored. \n",
    "                    r_rain,c_rain = [[i] for i in np.nonzero(did_rain[day])[0]],[i for i in np.nonzero(did_rain[day])[0]]\n",
    "                    r_dry,c_dry = [[i] for i in range(len_locs) if i not in np.nonzero(did_rain[day])[0]],[i for i in range(len_locs) if i not in np.nonzero(did_rain[day])[0]]\n",
    "                    \n",
    "                    # create submatricies\n",
    "                    submat11 = cov_mat[r_dry,c_dry]\n",
    "                    submat12 = cov_mat[r_dry,c_rain]\n",
    "                    submat21 = cov_mat[r_rain,c_dry]\n",
    "                    submat22 = cov_mat[r_rain,c_rain]\n",
    "                    #### MC estimate part      ####\n",
    "\n",
    "                    if len(submat22)!=1:\n",
    "                        sub_cov = (submat11-submat12@np.linalg.inv(submat22)@submat21)#.reshape(len(c_dry),len(c_dry))\n",
    "                    else: # submat22 is an integer, aka only one non-zero obs\n",
    "                        sub_cov = (submat11-submat12*np.linalg.inv(submat22)[0]*submat21)#.reshape(len(c_dry),len(c_dry))\n",
    "                    \n",
    "                    # Do MC approximation to cdf\n",
    "                    nll += np.log(mc_cdf([rvs[i][:len(c_dry)] for i in range(len(rvs))],sub_cov,[inv_us[day][k] for k in c_dry]))\n",
    "\n",
    "                    #### MC estimate part - end ####\n",
    "\n",
    "\n",
    "                    # compute and add the elegant cdf\n",
    "                    #if len(submat22.todense())==1:\n",
    "                    #    nll += scs.multivariate_normal.logcdf(x=[Invcdf_Us[day][k] for k in c_dry],mean=np.zeros(len(submat11.todense())),cov=(submat11-submat12*scipy.sparse.linalg.inv(submat22)[0]*submat21).todense())\n",
    "                    #else:\n",
    "                    #    nll += scs.multivariate_normal.logcdf(x=[Invcdf_Us[day][k] for k in c_dry],mean=np.zeros(len(submat11.todense())),cov=(submat11-submat12@scipy.sparse.linalg.inv(submat22)@submat21).todense())\n",
    "            \n",
    "            # denominator: multiplication of marginals - pdf for positive rain, cdf for truncated\n",
    "            for loc in range(len(Invcdf_Us[0])):\n",
    "                if did_rain[day][loc]==1:# wet: add pdf(invcdf(u_i))\n",
    "                    nll -= scs.norm.logpdf(Invcdf_Us[day][loc],loc=0,scale=1)\n",
    "                else:# dry: add cdf(invcdf(pi))\n",
    "                    nll -= np.log(truncation_pi[day][loc])\n",
    "            \n",
    "        return -nll\n",
    "\n",
    "    def nll_new(self,theta,Invcdf_Us,truncation_pi,did_rain,len_locs):\n",
    "        cov_mat = np.nan_to_num(rbf(self.L,gamma=theta),copy=False,nan=0)\n",
    "        nll=0\n",
    "        for day in tqdm(range(len(Invcdf_Us))): #eg [251,...,500]\n",
    "            \n",
    "            # numerator: joint pdf integrated over truncation - equivalent to elegant normal cdf in some cases. Check cases:\n",
    "            if np.sum(did_rain[day])==len(did_rain[day]): #all wet, just normal pdf\n",
    "                nll += scs.multivariate_normal.logpdf(x=Invcdf_Us[day],mean=np.zeros(len(Invcdf_Us[0])),cov=cov_mat)\n",
    "            else: # some dry -> use elegant cdf\n",
    "                #print('elegant'+str(np.sum(did_rain[day])))\n",
    "                if np.sum(did_rain[day])==0: # all dry -> use normal cdf\n",
    "                    nll += scs.multivariate_normal.logcdf(x=Invcdf_Us[day],mean=np.zeros(len(Invcdf_Us[0])),cov=cov_mat)\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    # need to re-arrange the covariance matrix into 2*2 blocks based on censored/uncensored. \n",
    "                    r_rain,c_rain = [[i] for i in np.nonzero(did_rain[day])[0]],[i for i in np.nonzero(did_rain[day])[0]]\n",
    "                    r_dry,c_dry = [[i] for i in range(len_locs) if i not in np.nonzero(did_rain[day])[0]],[i for i in range(len_locs) if i not in np.nonzero(did_rain[day])[0]]\n",
    "                    \n",
    "                     # create submatricies\n",
    "                    submat11 = cov_mat[r_dry,c_dry]\n",
    "                    submat12 = cov_mat[r_dry,c_rain]\n",
    "                    submat21 = cov_mat[r_rain,c_dry]\n",
    "                    submat22 = cov_mat[r_rain,c_rain]\n",
    "                    \n",
    "                    #### New estimate part      ####\n",
    "\n",
    "                    if len(submat22)!=1:\n",
    "                        sub_cov = (submat11-submat12@np.linalg.inv(submat22)@submat21)#.reshape(len(c_dry),len(c_dry))\n",
    "                    else: # submat22 is an integer, aka only one non-zero obs\n",
    "                        sub_cov = (submat11-submat12*np.linalg.inv(submat22)[0]*submat21)#.reshape(len(c_dry),len(c_dry))\n",
    "                    \n",
    "                    # Do new approximation to cdf\n",
    "                    test = [c_dry,[Invcdf_Us[day][k] for k in c_dry],sub_cov]\n",
    "                    nll += mvn_cdf([-np.inf for i in range(len(c_dry))],[Invcdf_Us[day][k] for k in c_dry],sub_cov)                               \n",
    "\n",
    "                    #### New estimate part - end ####\n",
    "\n",
    "\n",
    "                    # compute and add the elegant cdf\n",
    "                    #if len(submat22.todense())==1:\n",
    "                    #    nll += scs.multivariate_normal.logcdf(x=[Invcdf_Us[day][k] for k in c_dry],mean=np.zeros(len(submat11.todense())),cov=(submat11-submat12*scipy.sparse.linalg.inv(submat22)[0]*submat21).todense())\n",
    "                    #else:\n",
    "                    #    nll += scs.multivariate_normal.logcdf(x=[Invcdf_Us[day][k] for k in c_dry],mean=np.zeros(len(submat11.todense())),cov=(submat11-submat12@scipy.sparse.linalg.inv(submat22)@submat21).todense())\n",
    "            \n",
    "            # denominator: multiplication of marginals - pdf for positive rain, cdf for truncated\n",
    "            for loc in range(len(Invcdf_Us[0])):\n",
    "                if did_rain[day][loc]==1:# wet: add pdf(invcdf(u_i))\n",
    "                    nll -= scs.norm.logpdf(Invcdf_Us[day][loc],loc=0,scale=1)\n",
    "                else:# dry: add cdf(invcdf(pi))\n",
    "                    nll -= np.log(truncation_pi[day][loc])\n",
    "            \n",
    "        return -nll\n",
    "\n",
    "#function for single nll eval   \n",
    "\n",
    "def nll_sub_days(days_list,Invcdf_Us,truncation_pi,did_rain,len_locs,cov_mat):\n",
    "    nll = 0\n",
    "    for day in tqdm(days_list): #eg [251,...,500]\n",
    "            \n",
    "        # numerator: joint pdf integrated over truncation - equivalent to elegant normal cdf in some cases. Check cases:\n",
    "        if np.sum(did_rain[day])==len(did_rain[day]): #all wet, just normal pdf\n",
    "            nll += scs.multivariate_normal.logpdf(x=Invcdf_Us[day],mean=np.zeros(len(Invcdf_Us[0])),cov=cov_mat)\n",
    "        else: # some dry -> use elegant cdf\n",
    "            #print('elegant'+str(np.sum(did_rain[day])))\n",
    "            if np.sum(did_rain[day])==0: # all dry -> use normal cdf\n",
    "                nll += scs.multivariate_normal.logcdf(x=Invcdf_Us[day],mean=np.zeros(len(Invcdf_Us[0])),cov=cov_mat)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                # need to re-arrange the covariance matrix into 2*2 blocks based on censored/uncensored. \n",
    "                r_rain,c_rain = [[i] for i in np.nonzero(did_rain[day])[0]],[i for i in np.nonzero(did_rain[day])[0]]\n",
    "                r_dry,c_dry = [[i] for i in range(len_locs) if i not in np.nonzero(did_rain[day])[0]],[i for i in range(len_locs) if i not in np.nonzero(did_rain[day])[0]]\n",
    "                \n",
    "                    # create submatricies\n",
    "                submat11 = cov_mat[r_dry,c_dry]\n",
    "                submat12 = cov_mat[r_dry,c_rain]\n",
    "                submat21 = cov_mat[r_rain,c_dry]\n",
    "                submat22 = cov_mat[r_rain,c_rain]\n",
    "                \n",
    "                #### New estimate part      ####\n",
    "\n",
    "                if len(submat22)!=1:\n",
    "                    sub_cov = (submat11-submat12@np.linalg.inv(submat22)@submat21)#.reshape(len(c_dry),len(c_dry))\n",
    "                else: # submat22 is an integer, aka only one non-zero obs\n",
    "                    sub_cov = (submat11-submat12*np.linalg.inv(submat22)[0]*submat21)#.reshape(len(c_dry),len(c_dry))\n",
    "                \n",
    "                # Do new approximation to cdf\n",
    "                plt.matshow(sub_cov)\n",
    "                print(sub_cov.min())\n",
    "                print(sub_cov.max())\n",
    "                test = [c_dry,[Invcdf_Us[day][k] for k in c_dry],sub_cov]\n",
    "                print(mvn_cdf([-np.inf for i in range(len(c_dry))],[Invcdf_Us[day][k] for k in c_dry],sub_cov))\n",
    "                nll += mvn_cdf([-np.inf for i in range(len(c_dry))],[Invcdf_Us[day][k] for k in c_dry],sub_cov)                             \n",
    "\n",
    "                #### New estimate part - end ####\n",
    "\n",
    "\n",
    "                # compute and add the elegant cdf\n",
    "                #if len(submat22.todense())==1:\n",
    "                #    nll += scs.multivariate_normal.logcdf(x=[Invcdf_Us[day][k] for k in c_dry],mean=np.zeros(len(submat11.todense())),cov=(submat11-submat12*scipy.sparse.linalg.inv(submat22)[0]*submat21).todense())\n",
    "                #else:\n",
    "                #    nll += scs.multivariate_normal.logcdf(x=[Invcdf_Us[day][k] for k in c_dry],mean=np.zeros(len(submat11.todense())),cov=(submat11-submat12@scipy.sparse.linalg.inv(submat22)@submat21).todense())\n",
    "        \n",
    "        # denominator: multiplication of marginals - pdf for positive rain, cdf for truncated\n",
    "        for loc in range(len(Invcdf_Us[0])):\n",
    "            if did_rain[day][loc]==1:# wet: add pdf(invcdf(u_i))\n",
    "                nll -= scs.norm.logpdf(Invcdf_Us[day][loc],loc=0,scale=1)\n",
    "            else:# dry: add cdf(invcdf(pi))\n",
    "                nll -= np.log(truncation_pi[day][loc])\n",
    "        \n",
    "    return [-nll,test]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Checkpoints/uk_rain_DGLM_HConvLSTM_tdscale_gamma_hurdle/lightning_logs/version_7/', 'Checkpoints/uk_rain_DGLM_HConvLSTM_tdscale_gamma_hurdle/lightning_logs/version_7/checkpoints/epoch=12-step=25572-val_loss_loss=-0.391-val_metric_mse_rain=21.531.ckpt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\u1819911\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\u1819911\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load gamma model\n",
    "\n",
    "model_version = 7\n",
    "target_distribution = \"gamma_hurdle\"\n",
    "base_nn = \"HConvLSTM_tdscale\" #(Keep this fixed)\n",
    "\n",
    "# Run this to get the prediction data in a variable named city_data\n",
    "# Getting data\n",
    "sys.path.append('../')\n",
    "import glms\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "path_ = f\"Checkpoints/uk_rain_DGLM_{base_nn}_{target_distribution}/lightning_logs/version_{model_version}/\"\n",
    "ckpt_path = glms.NeuralDGLM.get_ckpt_path(os.path.join(path_,\"checkpoints\"))\n",
    "print([path_,ckpt_path])\n",
    "scaler_features, scaler_target = glms.NeuralDGLM.load_scalers(path_)\n",
    "model = glms.NeuralDGLM.load_from_checkpoint(ckpt_path, save_hparams=False, scaler_features=scaler_features, scaler_target=scaler_target)\n",
    "model.freeze()\n",
    "model.eval()\n",
    "\n",
    "test_output = pickle.load( open(os.path.join(path_,\"test_output_2014_2019-07.pkl\"),\"rb\") )\n",
    "#########\n",
    "\n",
    "# create list of locations\n",
    "loc_lat_lon = list(test_output.keys())\n",
    "output_keys = ['pred_mu', 'pred_disp', 'target_did_rain', 'target_rain_value', 'date', 'pred_p']\n",
    "\n",
    "#'lat_50.95_49.35_lon_-4.55_-2.95' has no data after week indexed 255, so I did not consider it.\n",
    "loc_lat_lon.remove('lat_50.95_49.35_lon_-4.55_-2.95')\n",
    "\n",
    "#########\n",
    "\n",
    "# create list of idexes corresponding to land locations\n",
    "is_loc_land = [] # boolean vector indicating for each location if it is land\n",
    "                 #[0,1,...,0] length=#locations\n",
    "                 # if it is not land, that location is getting skipped in gamma_outputs\n",
    "                 # WILL NEED TO TAKE THIS INTO ACCOUNT ONCE I PLOT THE MAP\n",
    "for loc in loc_lat_lon:\n",
    "    for week in range(1):\n",
    "        for day in range(1):\n",
    "            for row in range(4):\n",
    "                for column in range(4):\n",
    "                    if test_output[loc]['mask'][week][day][row][column]==1:\n",
    "                        is_loc_land.append(1)\n",
    "                    else:\n",
    "                        is_loc_land.append(0)\n",
    "\n",
    "idx_land = []                       \n",
    "for idx,b in enumerate(is_loc_land):\n",
    "    if b==1:\n",
    "        idx_land.append(idx)\n",
    "\n",
    "#########\n",
    "\n",
    "gamma_outputs = [[[],[],[],[],[],[]] for places in range(len(loc_lat_lon))] \n",
    "for location_idx in range(len(loc_lat_lon)):\n",
    "    gamma_outputs[location_idx].append([float(loc_lat_lon[location_idx].split('_')[i]) for i in [1,2,4,5]])\n",
    "    gamma_outputs[location_idx].append(test_output[loc_lat_lon[location_idx]]['mask'][0][0])\n",
    "    for output_idx in range(6):\n",
    "        for week in range(286):\n",
    "            for day in range(7):\n",
    "                #check for date, as date is formatted differently from the rest.\n",
    "                if output_idx==4:\n",
    "                    if day==0:\n",
    "                        gamma_outputs[location_idx][output_idx].append(np.array(test_output[loc_lat_lon[location_idx]][output_keys[output_idx]][week],dtype='datetime64[D]'))\n",
    "                    else:\n",
    "                        gamma_outputs[location_idx][output_idx].append('')                   \n",
    "                else:\n",
    "                    gamma_outputs[location_idx][output_idx].append(test_output[loc_lat_lon[location_idx]][output_keys[output_idx]][week][day])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gamma_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50.95, 49.35, -5.75, -4.15]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_outputs[80][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data into [day][param][all_4087_locs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [01:55<00:00, 17.34it/s]\n"
     ]
    }
   ],
   "source": [
    "g_4087_data = [[[],[],[],[],[]] for i in range(2002)]\n",
    "\n",
    "for day in tqdm(range(2002)):\n",
    "    for old_loc in range(len(gamma_outputs)):\n",
    "        for param_idx,param in enumerate([0,1,2,3,5]):\n",
    "            for row in range(4):\n",
    "                for col in range(4):\n",
    "                    if gamma_outputs[old_loc][7][row][col]==1:\n",
    "                        g_4087_data[day][param_idx].append(gamma_outputs[old_loc][param][day][row][col])\n",
    "                        \n",
    "\n",
    "#Previous\n",
    "# 0 'pred_mu', 1'pred_disp', 2'target_did_rain', 3'target_rain_value', 4'date', 5'pred_p', 6'location'.\n",
    "#[place][parameter][day][row][column]\n",
    "#Old:\n",
    "#0:i,1:location,2:pred_mu,3:pred_disp,4:pred_p,5:target_rain_value,6:dates\n",
    "\n",
    "# Now:  0: mu , 1: disp , 2: did_rain , 3: obs_rin , 4: p , 5: [lat,lon]\n",
    "# [day][param][loc_4087]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_4087 = []\n",
    "\n",
    "for old_loc in range(len(gamma_outputs)):\n",
    "    for row in range(4):\n",
    "        for col in range(4):\n",
    "            loc_4087.append([gamma_outputs[old_loc][6][1]-0.6-0.1*col,gamma_outputs[old_loc][6][2]+0.6+0.1*row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5504/5504 [00:38<00:00, 144.15it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(loc_4087):\n",
    "    plt.scatter(i[1],i[0],color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.95, -5.65]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gamma_outputs[0][6][2]+0.6,gamma_outputs[1][6][3]-0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58.35, 58.25, 58.15, 58.050000000000004]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gamma_outputs[0][6][0]-0.6-0.1*i for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5.95, -5.8500000000000005, -5.75, -5.65]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[gamma_outputs[0][6][2]+0.6+0.1*i for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maped_loc = [[[],[],[],[]] for i in range(len(loc_lat_lon))]\n",
    "for i in range(len(loc_lat_lon)):\n",
    "    maped_loc[i][0] = m(3,gamma_outputs[i][6][0]-0.6)[1]\n",
    "    maped_loc[i][1] = m(3,gamma_outputs[i][6][1]+0.7)[1]\n",
    "    maped_loc[i][2] = m(gamma_outputs[i][6][2]+0.6,3)[0]\n",
    "    maped_loc[i][3] = m(gamma_outputs[i][6][3]-0.7,3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\u1819911\\Desktop\\Copula Work\\Copula code\\All UK model 1\\Sim_truncated_copula.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/u1819911/Desktop/Copula%20Work/Copula%20code/All%20UK%20model%201/Sim_truncated_copula.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mg_4087_data.txt\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/u1819911/Desktop/Copula%20Work/Copula%20code/All%20UK%20model%201/Sim_truncated_copula.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     pickle\u001b[39m.\u001b[39;49mdump(g_4087_data,f)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# save\n",
    "with open('g_4087_data.txt','wb') as f:\n",
    "    pickle.dump(g_4087_data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating rainfall values\n",
    "\n",
    "Take simulated us and get the ppf of the gamma mixture for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4087"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g2_us[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8878959315446191, 0.9349859146245704],\n",
       " [0.8878959315446191, 0.9055948861212759],\n",
       " [0.8878959315446191, 0.9055948861212759],\n",
       " [0.8878959315446191, 0.9055948861212759],\n",
       " [0.8878959315446191, 0.9055948861212759]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulate\n",
    "k = truncgauss(dist_mat[:2,:2]).sim(10,inv_us,5,day_idx=3)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_Finv(day_id,loc_id,u):\n",
    "    if 1-gamma_outputs[loc_id][5][day_id]>u: #if 1-p>u, invcdf is 0 rain\n",
    "        return 0\n",
    "    else: # invcdf( (u-1+p) /p)\n",
    "        return scs.gamma(scale=gamma_outputs[loc_id][3][day_id]*gamma_outputs[loc_id][2][day_id],a=1/gamma_outputs[loc_id][3][day_id]).ppf((u-1+gamma_outputs[loc_id][4][day_id])/gamma_outputs[loc_id][4][day_id])\n",
    "        \n",
    "\n",
    "#Here:\n",
    "# 0 'pred_mu', 1'pred_disp', 2'target_did_rain', 3'target_rain_value', 4'date', 5'pred_p', 6'location'.\n",
    "#[place][parameter][day][row][column]\n",
    "#Old:\n",
    "#0:i,1:location,2:pred_mu,3:pred_disp,4:pred_p,5:target_rain_value,6:dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Basemap(projection='mill',\n",
    "            resolution='i',llcrnrlat = 48,\n",
    "            llcrnrlon = -12,\n",
    "            urcrnrlat = 60,\n",
    "            urcrnrlon = 3)\n",
    "            \n",
    "maped_loc = [[[],[],[],[]] for i in range(len(loc_lat_lon))]\n",
    "for i in range(len(loc_lat_lon)):\n",
    "    maped_loc[i][0] = m(3,gamma_outputs[i][6][0])[1]\n",
    "    maped_loc[i][1] = m(3,gamma_outputs[i][6][1])[1]\n",
    "    maped_loc[i][2] = m(gamma_outputs[i][6][2],3)[0]\n",
    "    maped_loc[i][3] = m(gamma_outputs[i][6][3],3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab60c10c504ddb966051b34fe3c8284e278512e8f8393189136f471cdd0fd1b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
