{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with copulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a list of Us as follows:\n",
    " - For each location, we start with 16 observations corresponding to 16 sublocations. We treat these as seperate locations for the rest of this work.\n",
    " - The list of Us will be of length 2002 [day]. Each of these 2002 elements will be an array of length 4727 [Us] corresponding to the Us of each location for the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "%matplotlib inline\n",
    "from scipy.optimize import dual_annealing\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from collections import Counter\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "from scipy.spatial import distance_matrix\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Checkpoints/uk_rain_DGLM_HConvLSTM_tdscale_gamma_hurdle/lightning_logs/version_1/', 'Checkpoints/uk_rain_DGLM_HConvLSTM_tdscale_gamma_hurdle/lightning_logs/version_1/checkpoints/epoch=11-step=22556-val_loss_loss=-0.349-val_metric_mse_rain=22.651.ckpt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\david\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.23.2 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### load gamma\n",
    "model_version = 1\n",
    "target_distribution = \"gamma_hurdle\"\n",
    "base_nn = \"HConvLSTM_tdscale\" #(Keep this fixed)\n",
    "\n",
    "# Run this to get the prediction data in a variable named city_data\n",
    "# Getting data\n",
    "sys.path.append('../')\n",
    "import glms\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "path_ = f\"Checkpoints/uk_rain_DGLM_{base_nn}_{target_distribution}/lightning_logs/version_{model_version}/\"\n",
    "ckpt_path = glms.NeuralDGLM.get_ckpt_path(os.path.join(path_,\"checkpoints\"))\n",
    "print([path_,ckpt_path])\n",
    "scaler_features, scaler_target = glms.NeuralDGLM.load_scalers(path_)\n",
    "model = glms.NeuralDGLM.load_from_checkpoint(ckpt_path, save_hparams=False, scaler_features=scaler_features, scaler_target=scaler_target)\n",
    "model.freeze()\n",
    "model.eval()\n",
    "\n",
    "test_output = pickle.load( open(os.path.join(path_,\"test_output_2014_2019-07.pkl\"),\"rb\") )\n",
    "#########\n",
    "\n",
    "# create list of locations\n",
    "loc_lat_lon = ['lat_57.75_56.15_lon_-5.75_-4.15', 'lat_53.75_52.15_lon_-3.35_-1.75', 'lat_53.75_52.15_lon_-2.95_-1.35', 'lat_55.75_54.15_lon_-5.35_-3.75', 'lat_55.75_54.15_lon_-4.95_-3.35', 'lat_53.35_51.75_lon_-0.95_0.65', 'lat_52.95_51.35_lon_-10.95_-9.35', 'lat_54.95_53.35_lon_-9.75_-8.15', 'lat_54.95_53.35_lon_-9.35_-7.75', 'lat_51.75_50.15_lon_-4.95_-3.35', 'lat_51.75_50.15_lon_-4.55_-2.95', 'lat_54.55_52.95_lon_-5.75_-4.15', 'lat_54.55_52.95_lon_-5.35_-3.75', 'lat_51.35_49.75_lon_-0.55_1.05', 'lat_51.35_49.75_lon_-0.15_1.45', 'lat_54.15_52.55_lon_-2.55_-0.95', 'lat_54.15_52.55_lon_-2.15_-0.55', 'lat_57.35_55.75_lon_-3.75_-2.15', 'lat_57.35_55.75_lon_-3.35_-1.75', 'lat_53.35_51.75_lon_-10.95_-9.35', 'lat_53.35_51.75_lon_-10.55_-8.95', 'lat_55.35_53.75_lon_-10.95_-9.35', 'lat_55.35_53.75_lon_-10.55_-8.95', 'lat_52.95_51.35_lon_-8.55_-6.95', 'lat_52.95_51.35_lon_-8.15_-6.55', 'lat_54.95_53.35_lon_-6.95_-5.35', 'lat_54.95_53.35_lon_-6.55_-4.95', 'lat_51.75_50.15_lon_-2.15_-0.55', 'lat_51.75_50.15_lon_-1.75_-0.15', 'lat_54.15_52.55_lon_-9.35_-7.75', 'lat_54.15_52.55_lon_-8.95_-7.35', 'lat_57.75_56.15_lon_-5.35_-3.75', 'lat_53.75_52.15_lon_-2.55_-0.95', 'lat_55.75_54.15_lon_-4.55_-2.95', 'lat_52.95_51.35_lon_-10.55_-8.95', 'lat_54.95_53.35_lon_-8.95_-7.35', 'lat_51.75_50.15_lon_-4.15_-2.55', 'lat_54.55_52.95_lon_-4.95_-3.35', 'lat_51.35_49.75_lon_0.25_1.85', 'lat_54.15_52.55_lon_-1.75_-0.15', 'lat_56.95_55.35_lon_-5.75_-4.15', 'lat_53.35_51.75_lon_-10.15_-8.55', 'lat_55.35_53.75_lon_-10.15_-8.55', 'lat_52.95_51.35_lon_-7.75_-6.15', 'lat_54.95_53.35_lon_-6.15_-4.55', 'lat_51.75_50.15_lon_-1.35_0.25', 'lat_54.15_52.55_lon_-8.55_-6.95', 'lat_57.35_55.75_lon_-5.75_-4.15', 'lat_53.75_52.15_lon_-2.15_-0.55', 'lat_55.75_54.15_lon_-4.15_-2.55', 'lat_52.95_51.35_lon_-10.15_-8.55', 'lat_54.95_53.35_lon_-8.55_-6.95', 'lat_51.75_50.15_lon_-3.75_-2.15', 'lat_54.55_52.95_lon_-4.55_-2.95', 'lat_51.35_49.75_lon_0.65_2.25', 'lat_54.15_52.55_lon_-1.35_0.25', 'lat_56.95_55.35_lon_-5.35_-3.75', 'lat_53.35_51.75_lon_-9.75_-8.15', 'lat_55.35_53.75_lon_-9.75_-8.15', 'lat_52.95_51.35_lon_-7.35_-5.75', 'lat_54.95_53.35_lon_-5.75_-4.15', 'lat_51.75_50.15_lon_-0.95_0.65', 'lat_54.15_52.55_lon_-8.15_-6.55', 'lat_57.35_55.75_lon_-5.35_-3.75', 'lat_53.75_52.15_lon_-1.75_-0.15', 'lat_55.75_54.15_lon_-3.75_-2.15', 'lat_52.95_51.35_lon_-9.75_-8.15', 'lat_54.95_53.35_lon_-8.15_-6.55', 'lat_51.75_50.15_lon_-3.35_-1.75', 'lat_54.55_52.95_lon_-4.15_-2.55', 'lat_51.35_49.75_lon_1.05_2.65', 'lat_54.15_52.55_lon_-0.95_0.65', 'lat_56.95_55.35_lon_-4.95_-3.35', 'lat_53.35_51.75_lon_-9.35_-7.75', 'lat_55.35_53.75_lon_-9.35_-7.75', 'lat_52.95_51.35_lon_-6.95_-5.35', 'lat_54.95_53.35_lon_-5.35_-3.75', 'lat_51.75_50.15_lon_-0.55_1.05', 'lat_54.15_52.55_lon_-7.75_-6.15', 'lat_57.35_55.75_lon_-4.95_-3.35', 'lat_53.75_52.15_lon_-1.35_0.25', 'lat_55.75_54.15_lon_-3.35_-1.75', 'lat_52.95_51.35_lon_-9.35_-7.75', 'lat_54.95_53.35_lon_-7.75_-6.15', 'lat_51.75_50.15_lon_-2.95_-1.35', 'lat_54.55_52.95_lon_-3.75_-2.15', 'lat_50.95_49.35_lon_-6.55_-4.95', 'lat_53.75_52.15_lon_-10.95_-9.35', 'lat_56.95_55.35_lon_-4.55_-2.95', 'lat_53.35_51.75_lon_-8.95_-7.35', 'lat_55.35_53.75_lon_-8.95_-7.35', 'lat_52.95_51.35_lon_-6.55_-4.95', 'lat_54.95_53.35_lon_-4.95_-3.35', 'lat_51.75_50.15_lon_-0.15_1.45', 'lat_54.15_52.55_lon_-7.35_-5.75', 'lat_57.35_55.75_lon_-4.55_-2.95', 'lat_53.75_52.15_lon_-0.95_0.65', 'lat_55.75_54.15_lon_-2.95_-1.35', 'lat_52.95_51.35_lon_-8.95_-7.35', 'lat_54.95_53.35_lon_-7.35_-5.75', 'lat_51.75_50.15_lon_-2.55_-0.95', 'lat_54.55_52.95_lon_-3.35_-1.75', 'lat_50.95_49.35_lon_-6.15_-4.55', 'lat_53.75_52.15_lon_-10.55_-8.95', 'lat_56.95_55.35_lon_-4.15_-2.55', 'lat_53.35_51.75_lon_-8.55_-6.95', 'lat_55.35_53.75_lon_-8.55_-6.95', 'lat_52.95_51.35_lon_-6.15_-4.55', 'lat_54.95_53.35_lon_-4.55_-2.95', 'lat_51.75_50.15_lon_0.25_1.85', 'lat_54.15_52.55_lon_-6.95_-5.35', 'lat_57.35_55.75_lon_-4.15_-2.55', 'lat_54.55_52.95_lon_-2.95_-1.35', 'lat_50.95_49.35_lon_-5.75_-4.15', 'lat_53.75_52.15_lon_-10.15_-8.55', 'lat_56.95_55.35_lon_-3.75_-2.15', 'lat_53.35_51.75_lon_-8.15_-6.55', 'lat_55.35_53.75_lon_-8.15_-6.55', 'lat_52.95_51.35_lon_-5.75_-4.15', 'lat_54.95_53.35_lon_-4.15_-2.55', 'lat_51.75_50.15_lon_0.65_2.25', 'lat_54.15_52.55_lon_-6.55_-4.95', 'lat_54.55_52.95_lon_-2.55_-0.95', 'lat_50.95_49.35_lon_-5.35_-3.75', 'lat_53.75_52.15_lon_-9.75_-8.15', 'lat_56.95_55.35_lon_-3.35_-1.75', 'lat_53.35_51.75_lon_-7.75_-6.15', 'lat_55.35_53.75_lon_-7.75_-6.15', 'lat_52.95_51.35_lon_-5.35_-3.75', 'lat_54.95_53.35_lon_-3.75_-2.15', 'lat_51.75_50.15_lon_1.05_2.65', 'lat_54.15_52.55_lon_-6.15_-4.55', 'lat_54.55_52.95_lon_-2.15_-0.55', 'lat_50.95_49.35_lon_-4.95_-3.35', 'lat_53.75_52.15_lon_-9.35_-7.75', 'lat_56.55_54.95_lon_-5.75_-4.15', 'lat_53.35_51.75_lon_-7.35_-5.75', 'lat_55.35_53.75_lon_-7.35_-5.75', 'lat_52.95_51.35_lon_-4.95_-3.35', 'lat_54.95_53.35_lon_-3.35_-1.75', 'lat_51.35_49.75_lon_-6.55_-4.95', 'lat_54.15_52.55_lon_-5.75_-4.15', 'lat_54.15_52.55_lon_-10.95_-9.35', 'lat_50.95_49.35_lon_-4.55_-2.95', 'lat_53.75_52.15_lon_-8.95_-7.35', 'lat_56.55_54.95_lon_-5.35_-3.75', 'lat_53.35_51.75_lon_-6.95_-5.35', 'lat_55.35_53.75_lon_-6.95_-5.35', 'lat_52.95_51.35_lon_-4.55_-2.95', 'lat_54.95_53.35_lon_-2.95_-1.35', 'lat_51.35_49.75_lon_-6.15_-4.55', 'lat_54.15_52.55_lon_-5.35_-3.75', 'lat_54.15_52.55_lon_-10.55_-8.95', 'lat_50.95_49.35_lon_-4.15_-2.55', 'lat_53.75_52.15_lon_-8.55_-6.95', 'lat_56.55_54.95_lon_-4.95_-3.35', 'lat_53.35_51.75_lon_-6.55_-4.95', 'lat_55.35_53.75_lon_-6.55_-4.95', 'lat_52.95_51.35_lon_-4.15_-2.55', 'lat_54.95_53.35_lon_-2.55_-0.95', 'lat_51.35_49.75_lon_-5.75_-4.15', 'lat_54.15_52.55_lon_-4.95_-3.35', 'lat_54.15_52.55_lon_-10.15_-8.55', 'lat_50.95_49.35_lon_-3.75_-2.15', 'lat_53.75_52.15_lon_-8.15_-6.55', 'lat_56.55_54.95_lon_-4.55_-2.95', 'lat_53.35_51.75_lon_-6.15_-4.55', 'lat_55.35_53.75_lon_-6.15_-4.55', 'lat_52.95_51.35_lon_-3.75_-2.15', 'lat_54.55_52.95_lon_-10.95_-9.35', 'lat_51.35_49.75_lon_-5.35_-3.75', 'lat_54.15_52.55_lon_-4.55_-2.95', 'lat_54.15_52.55_lon_-9.75_-8.15', 'lat_50.95_49.35_lon_-3.35_-1.75', 'lat_53.75_52.15_lon_-7.75_-6.15', 'lat_56.55_54.95_lon_-4.15_-2.55', 'lat_53.35_51.75_lon_-5.75_-4.15', 'lat_55.35_53.75_lon_-5.75_-4.15', 'lat_52.95_51.35_lon_-3.35_-1.75', 'lat_54.55_52.95_lon_-10.55_-8.95', 'lat_51.35_49.75_lon_-4.95_-3.35', 'lat_54.15_52.55_lon_-4.15_-2.55', 'lat_50.95_49.35_lon_-2.95_-1.35', 'lat_53.75_52.15_lon_-7.35_-5.75', 'lat_56.55_54.95_lon_-3.75_-2.15', 'lat_53.35_51.75_lon_-5.35_-3.75', 'lat_55.35_53.75_lon_-5.35_-3.75', 'lat_52.95_51.35_lon_-2.95_-1.35', 'lat_54.55_52.95_lon_-10.15_-8.55', 'lat_51.35_49.75_lon_-4.55_-2.95', 'lat_54.15_52.55_lon_-3.75_-2.15', 'lat_50.95_49.35_lon_-2.55_-0.95', 'lat_53.75_52.15_lon_-6.95_-5.35', 'lat_56.55_54.95_lon_-3.35_-1.75', 'lat_53.35_51.75_lon_-4.95_-3.35', 'lat_55.35_53.75_lon_-4.95_-3.35', 'lat_52.95_51.35_lon_-2.55_-0.95', 'lat_54.55_52.95_lon_-9.75_-8.15', 'lat_51.35_49.75_lon_-4.15_-2.55', 'lat_54.15_52.55_lon_-3.35_-1.75', 'lat_54.15_52.55_lon_-2.95_-1.35', 'lat_50.95_49.35_lon_-2.15_-0.55', 'lat_53.75_52.15_lon_-6.55_-4.95', 'lat_56.15_54.55_lon_-5.75_-4.15', 'lat_53.35_51.75_lon_-4.55_-2.95', 'lat_55.35_53.75_lon_-4.55_-2.95', 'lat_52.95_51.35_lon_-2.15_-0.55', 'lat_54.55_52.95_lon_-9.35_-7.75', 'lat_51.35_49.75_lon_-3.75_-2.15', 'lat_50.95_49.35_lon_-1.75_-0.15', 'lat_53.75_52.15_lon_-6.15_-4.55', 'lat_56.15_54.55_lon_-5.35_-3.75', 'lat_53.35_51.75_lon_-4.15_-2.55', 'lat_55.35_53.75_lon_-4.15_-2.55', 'lat_52.95_51.35_lon_-1.75_-0.15', 'lat_54.55_52.95_lon_-8.95_-7.35', 'lat_51.35_49.75_lon_-3.35_-1.75', 'lat_50.95_49.35_lon_-1.35_0.25', 'lat_53.75_52.15_lon_-5.75_-4.15', 'lat_56.15_54.55_lon_-4.95_-3.35', 'lat_53.35_51.75_lon_-3.75_-2.15', 'lat_55.35_53.75_lon_-3.75_-2.15', 'lat_52.95_51.35_lon_-1.35_0.25', 'lat_54.55_52.95_lon_-8.55_-6.95', 'lat_51.35_49.75_lon_-2.95_-1.35', 'lat_50.95_49.35_lon_-0.95_0.65', 'lat_53.75_52.15_lon_-5.35_-3.75', 'lat_56.15_54.55_lon_-4.55_-2.95', 'lat_53.35_51.75_lon_-3.35_-1.75', 'lat_55.35_53.75_lon_-3.35_-1.75', 'lat_52.95_51.35_lon_-0.95_0.65', 'lat_54.55_52.95_lon_-8.15_-6.55', 'lat_51.35_49.75_lon_-2.55_-0.95', 'lat_50.95_49.35_lon_-0.55_1.05', 'lat_53.75_52.15_lon_-4.95_-3.35', 'lat_56.15_54.55_lon_-4.15_-2.55', 'lat_53.35_51.75_lon_-2.95_-1.35', 'lat_55.35_53.75_lon_-2.95_-1.35', 'lat_51.75_50.15_lon_-6.55_-4.95', 'lat_54.55_52.95_lon_-7.75_-6.15', 'lat_51.35_49.75_lon_-2.15_-0.55', 'lat_50.95_49.35_lon_-0.15_1.45', 'lat_53.75_52.15_lon_-4.55_-2.95', 'lat_56.15_54.55_lon_-3.75_-2.15', 'lat_53.35_51.75_lon_-2.55_-0.95', 'lat_54.95_53.35_lon_-10.95_-9.35', 'lat_51.75_50.15_lon_-6.15_-4.55', 'lat_54.55_52.95_lon_-7.35_-5.75', 'lat_51.35_49.75_lon_-1.75_-0.15', 'lat_50.95_49.35_lon_0.25_1.85', 'lat_53.75_52.15_lon_-4.15_-2.55', 'lat_56.15_54.55_lon_-3.35_-1.75', 'lat_53.35_51.75_lon_-2.15_-0.55', 'lat_54.95_53.35_lon_-10.55_-8.95', 'lat_51.75_50.15_lon_-5.75_-4.15', 'lat_54.55_52.95_lon_-6.95_-5.35', 'lat_51.35_49.75_lon_-1.35_0.25', 'lat_50.95_49.35_lon_0.65_2.25', 'lat_53.75_52.15_lon_-3.75_-2.15', 'lat_55.75_54.15_lon_-5.75_-4.15', 'lat_53.35_51.75_lon_-1.75_-0.15', 'lat_54.95_53.35_lon_-10.15_-8.55', 'lat_51.75_50.15_lon_-5.35_-3.75', 'lat_54.55_52.95_lon_-6.55_-4.95', 'lat_51.35_49.75_lon_-0.95_0.65', 'lat_53.35_51.75_lon_-1.35_0.25', 'lat_54.55_52.95_lon_-6.15_-4.55']\n",
    "output_keys = ['pred_mu', 'pred_disp', 'target_did_rain', 'target_rain_value', 'date', 'pred_p']\n",
    "#  'pred_mu', 'pred_disp', 'target_did_rain', 'target_rain_value', 'date', 'pred_p'\n",
    "\n",
    "# formatting data\n",
    "gamma_outputs = [[[],[],[],[],[],[]] for places in range(len(loc_lat_lon))] \n",
    "for location_idx in range(len(loc_lat_lon)):\n",
    "    gamma_outputs[location_idx].append([float(loc_lat_lon[location_idx].split('_')[i]) for i in [1,2,4,5]])\n",
    "    for output_idx in range(6):\n",
    "        for week in range(286):\n",
    "            for day in range(7):\n",
    "                #check for date, as date is formatted differently from the rest.\n",
    "                if output_idx==4:\n",
    "                    if day==0:\n",
    "                        gamma_outputs[location_idx][output_idx].append(np.array(test_output[loc_lat_lon[location_idx]][output_keys[output_idx]][week],dtype='datetime64[D]'))\n",
    "                    else:\n",
    "                        gamma_outputs[location_idx][output_idx].append('')                   \n",
    "                else:\n",
    "                    gamma_outputs[location_idx][output_idx].append(test_output[loc_lat_lon[location_idx]][output_keys[output_idx]][week][day])\n",
    "\n",
    "\n",
    "# Loading data for Us\n",
    "with open('g_us.txt','rb') as f:\n",
    "    g_us = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copula classes\n",
    "\n",
    "# Gaussian\n",
    "# This class is with sigma fixed at 1 - This is what is used for the rest of this notebook\n",
    "class norm_cop_loconly():\n",
    "\n",
    "    def __init__(self,L=np.array([[0,1,1],[1,0,1],[1,1,0]])):\n",
    "        '''\n",
    "        L is location matrix: n x n\n",
    "        '''\n",
    "        self.N=L.shape[0]\n",
    "        self.L=L\n",
    "\n",
    "    def sim(self,theta,draws=3,as_x=True):\n",
    "        '''\n",
    "        Returns: [ [m [draws of length n] for day t+1],..., [m [draws of length n] for day t+T_(depending on prediction matricies X_all)] ]\n",
    "        So to get the samples from day t+1: sim[1-1]\n",
    "        To get k th draw from samples at day t+T: sim[T-1][k-1]\n",
    "        To get u for 3rd location of kth drw for day T: sim[T-1][k-1][3-1]'''\n",
    "        cov_mat=Matern(length_scale=theta[0],nu=theta[1]).__call__(self.L) *np.power(1,2)  \n",
    "        #sigma should be in theta ##############################\n",
    "\n",
    "        if as_x==True:\n",
    "            return [scs.multivariate_normal.rvs(mean=np.zeros(self.N),cov=cov_mat) for i in range(draws) ] \n",
    "        if as_x==False:\n",
    "            Us=[np.zeros(self.N) for i in range(draws)] \n",
    "            for i in range(draws):\n",
    "                Us[i]=[scs.multivariate_normal.cdf(y,mean=0,cov=cov_mat[idx,idx]) for idx,y in enumerate(scs.multivariate_normal.rvs(mean=np.zeros(self.N),cov=cov_mat))]\n",
    "            return Us\n",
    "\n",
    "    \n",
    "    def nll(self,theta,data):\n",
    "        '''\n",
    "        theta: [sigma,p0,v0,p1,v1] vector of parameters for the Matern Kernel\n",
    "        data: [[u1,..,un],...,[...]] one set of (n: one for each location) us for each day 1 to T (T is the number of predictor matricies).\n",
    "        '''\n",
    "        nll=0\n",
    "        cov_mat=Matern(length_scale=theta[0],nu=theta[1]).__call__(self.L) *np.power(1,2) \n",
    "        #sigma should be in theta ##############################\n",
    "\n",
    "        for day in range(len(data)):\n",
    "            #nll for the copula density: joint and marginals\n",
    "            nll+=scs.multivariate_normal.logpdf([scs.norm.ppf(obs,loc=0,scale=cov_mat[idx,idx]) for idx,obs in enumerate(data[day])],mean=np.zeros(self.N),cov=cov_mat) \n",
    "            nll+=-np.sum([scs.norm.logpdf(obs,loc=0,scale=cov_mat[idx,idx]) for idx,obs in enumerate(data[day])])\n",
    "        return -nll\n",
    "\n",
    "# Clayton\n",
    "class Clayton():\n",
    "    def __init__(self,n=2):\n",
    "        '''\n",
    "        Class to fit and work with a Clayton copula, using two functions.\n",
    "        sim: simulated [0,1] values using this copula density.\n",
    "        eval_nll: evaluate the negative log likelihood based on given data.\n",
    "        To use the class, the dimension of the data is needed.\n",
    "        n: dimension of data\n",
    "        '''\n",
    "        self.n=n # dimension\n",
    "\n",
    "    def sim(draws=10,theta=1):\n",
    "        '''\n",
    "        Simulates m draws from the Clayton copula density\n",
    "        '''\n",
    "        # not sure how yet.\n",
    "\n",
    "    def eval_nll(self,theta=10,data=[[0.4,0.6],[0.001,0.01],[0.1,0.1],[0.8,0.9],[0.1,0.0001],[0.3,0.1]]):\n",
    "        '''\n",
    "        theta: the parameter of the Clayton copula in [0,inf).\n",
    "        \n",
    "        data: [[u1_(k=1) ,..., un_(k=1)] ,..., [u1_(k=k) ,..., un(k=k)]] one set of (n of them: one for each location) us for each day 1 to T.\n",
    "        '''\n",
    "        # Compute the pdf by looping over each day, and adding the nll for that day to the result. \n",
    "\n",
    "        #### second atempt\n",
    "        nll=0\n",
    "        n=self.n\n",
    "\n",
    "        for day_data in data:\n",
    "            nll += n*(1+theta) + (n+(1/theta)) * np.log(1-n + np.sum(np.power(day_data,-theta)))\n",
    "            nll += -np.sum([np.log(u) + np.log(j*theta +1) for j,u in enumerate(day_data)])\n",
    "        \n",
    "        return nll\n",
    "\n",
    "# Bardossy2011 I-123 copula\n",
    "class cop_bardossy():\n",
    "\n",
    "    def __init__(self,L=np.array([[0,1,1],[1,0,1],[1,1,0]])):\n",
    "        '''\n",
    "        L is location matrix: n x n\n",
    "        '''\n",
    "        self.L=L\n",
    "    \n",
    "    def nll(self,theta,Us_didrain):\n",
    "        '''\n",
    "        theta: [sigma,p0,v0,p1,v1] vector of parameters for the Matern Kernel\n",
    "        Us: [[u1,..,un],...,[...]] one set of (n: one for each location) us for each day 1 to T (T is the number of predictor matricies).\n",
    "        didrain: 0/1 values in the same structure as Us, indicators of rain (1=rain).\n",
    "        '''\n",
    "        nll = 0\n",
    "        Us = Us_didrain[0]\n",
    "        didrain = Us_didrain[1]\n",
    "\n",
    "        # transform Us into invNorms\n",
    "        norm_us = [[] for i in range(1977) ]\n",
    "        for day in range(1977):\n",
    "            norm_us[day]=[scs.norm.ppf(u) for u in Us[day]]\n",
    "        \n",
    "        # kernel\n",
    "        cov_mat=np.nan_to_num(RBF(length_scale=theta).__call__(self.L),copy=False,nan=0)\n",
    "\n",
    "\n",
    "        #loglikelihood\n",
    "        for day in range(1977):\n",
    "            for pair in itertools.combinations(range(13),2):\n",
    "                if pair[0]+pair[1]==0: #I1\n",
    "                    nll += scs.multivariate_normal.logpdf(norm_us[day][pair[0]],norm_us[day][pair[1]],cov_mat[pair[0]][pair[1]])\n",
    "                if pair[0]+pair[1]==1: #I2 - have [1or0]*ll + [0or1]*ll to add the correct amount, depending on where it rained.\n",
    "                    nll += (didrain[day][pair[1]])*scs.norm.logpdf((norm_us[day][pair[0]]-norm_us[day][pair[1]]*cov_mat[pair[0]][pair[1]])/(sqrt(1-np.power(cov_mat[pair[0]][pair[1]],2)))) + (1-didrain[day][pair[1]])*scs.norm.logpdf((norm_us[day][pair[1]]-norm_us[day][pair[0]]*cov_mat[pair[0]][pair[1]])/(sqrt(1-np.power(cov_mat[pair[0]][pair[1]],2)))) \n",
    "                    nll += (didrain[day][pair[1]])*scs.norm.logpdf(norm_us[day][pair[0]]) + (1-didrain[day][pair[1]])*scs.norm.logpdf(norm_us[day][pair[1]])\n",
    "                if pair[0]+pair[1]==0: #I3\n",
    "                    nll += scs.multivariate_normal.logcdf(norm_us[day][pair[0]],norm_us[day][pair[1]],cov_mat[pair[0]][pair[1]])\n",
    "        \n",
    "        if math.isnan(nll):\n",
    "            nll = -9999999\n",
    "        return -nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance matrix for kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clayton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317846621.5479513"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clayton(n=len(g_us[0])).eval_nll(theta=10,data=g_us) # takes 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fval: 14326075.26123136 theta: 0.00806025 time: 1h\n",
    "dual_annealing(Clayton(n=len(g_us[0])).eval_nll,bounds=[[0.00000001,10]],args=[g_us],maxiter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bardossy 2011 copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating censored us\n",
    "\n",
    "# 0 'pred_mu', 1'pred_disp', 2'target_did_rain', 3'target_rain_value', 4'date', 5'pred_p', 6'location'.\n",
    "#[place][parameter][day][row][column]\n",
    "#0:i,1:location,2:pred_mu,3:pred_disp,4:pred_p,5:target_rain_value,6:dates\n",
    "\n",
    "censored_us = [[] for i in range(2002)]\n",
    "for day in range(2002):\n",
    "    for location in range(267):\n",
    "        for row in range(4):\n",
    "            for column in range(4):\n",
    "                censored_us[day].append(gamma_outputs[location][2][day][row][column])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need detail on coordinates to create a distance matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72f235a78b5cf937fd09c1593b6a0e4473f824a03930b62c2c7d9a177b9de8f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
