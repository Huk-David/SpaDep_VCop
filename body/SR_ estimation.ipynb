{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################### Loading packages ####################################################################\n",
    "import sys as sys\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math \n",
    "import re\n",
    "from Censored_copula import *\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics.pairwise import euclidean_distances \n",
    "import pandas as pd\n",
    "from scipy.optimize import dual_annealing\n",
    "from joblib import Parallel, delayed\n",
    "import psutil\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "\n",
    "#################################################################### Loading GNM ####################################################################\n",
    "model_version = 0 \n",
    "target_distribution = \"gamma_hurdle\"\n",
    "base_nn = \"HConvLSTM_tdscale\"\n",
    "sys.path.append('../')\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "path_ = f\"Checkpoints/uk_rain_DGLM_{base_nn}_{target_distribution}/lightning_logs/version_{model_version}/\"\n",
    "#ckpt_path = glms.NeuralDGLM.get_ckpt_path(os.path.join(path_,\"checkpoints\"))\n",
    "print(path_)\n",
    "GNM_output = pickle.load( open(os.path.join(path_,\".pkl\"),\"rb\") )\n",
    "\n",
    "#################################################################### Problematic locations? ###############################################################\n",
    "\n",
    "location_keys = list(GNM_output.keys())                 # '([0, 16], [44, 60])', ...\n",
    "output_keys = list(GNM_output[location_keys[0]].keys()) # 0'pred_mu', 1'pred_disp', 2'target_did_rain', 3'target_rain_value', 4'date', 5'pred_p', 6'mask', 7'idx_loc_in_region'\n",
    "\n",
    "\n",
    "# May have to remove a location that doesn't have the correct number of days:\n",
    "#location_keys.remove(location_keys[344]) #to remove for gamma_20_20\n",
    "#location_keys.remove(location_keys[171]) # to remove for gamma_20_20_unseen\n",
    "#location_keys.remove(location_keys[344]) # to remove for 10 y train\n",
    "#location_keys.remove(location_keys[312])  # to remove for 2y_test\n",
    "#location_keys.remove(location_keys[336]) #to remove for gamma_20_20 train  \n",
    "#################################################################### Utility functions ####################################################################\n",
    "def give_lat_lon(location_key):\n",
    "    return [ int(re.findall(r'\\d+',string)[0]) for string in location_key.split(',')]\n",
    "\n",
    "def is_loc_land(location,row,column):\n",
    "    return GNM_output[location]['mask'][0][row][column]\n",
    "\n",
    "#location_keys.remove('lat_50.95_49.35_lon_-4.55_-2.95') # 'lat_50.95_49.35_lon_-4.55_-2.95' has no data after week indexed 255, so I did not consider it.\n",
    "day_dates = [pd.to_datetime(i).date() for i in GNM_output[location_keys[0]]['date']]\n",
    "output_keys.remove('idx_loc_in_region') # not usefull\n",
    "output_keys.remove('date') # no more usefull\n",
    "\n",
    "#################################################################### Making list for land locations ####################################################################\n",
    "#GNM_landonly[location][param][day]\n",
    "# 0'pred_mu', 1'pred_disp', 2'target_did_rain', 3'target_rain_value', 4'pred_p', 5'location'\n",
    "\n",
    "GNM_landonly = [] \n",
    "land_counter = -1\n",
    "for location in tqdm(location_keys):\n",
    "    for col in range(4):\n",
    "        for row in range(4):\n",
    "            if not is_loc_land(location,row,col):#check if land\n",
    "               continue  \n",
    "            land_counter+=1\n",
    "            GNM_landonly.append([[],[],[],[],[]]) # parameters except location\n",
    "            temp_lat_lon = give_lat_lon(location)\n",
    "            GNM_landonly[land_counter].append([temp_lat_lon[1]-8+row,(temp_lat_lon[2])+6+col]) #location\n",
    "            for param_idx,param in enumerate(output_keys[:-1]):\n",
    "                for day in range(len(GNM_output[location_keys[0]]['pred_mu'])):\n",
    "                    qqq = [location,param,day,row,col]\n",
    "                    GNM_landonly[land_counter][param_idx].append(GNM_output[location][param][day][row,col])\n",
    "\n",
    "dist_mat = distance_matrix(np.matrix([[GNM_landonly[loc][5][0],GNM_landonly[loc][5][1]] for loc in range(len(GNM_landonly))]),np.matrix([[GNM_landonly[loc][5][0],GNM_landonly[loc][5][1]] for loc in range(len(GNM_landonly))]))\n",
    "\n",
    "#################################################################### Elevation data ####################################################################\n",
    "import netCDF4 as nc\n",
    "fn = 'topo_0.1_degree.nc'\n",
    "ds = nc.Dataset(fn)\n",
    "h = ds['Band1'][:]\n",
    "h = np.flip(np.matrix(h),axis=0)\n",
    "topo_dist = euclidean_distances(np.array([h[loc[5][0],loc[5][1]+2] for loc in GNM_landonly]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,loc in enumerate(location_keys):\n",
    "    if loc=='([80, 96], [64, 80])':\n",
    "        print(i,loc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting to make sure everything is in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check rain is correct\n",
    "for day in range(10,11):\n",
    "    fig = plt.gcf() \n",
    "    fig.set_size_inches(14,10)\n",
    "\n",
    "    plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(GNM_landonly))],[-GNM_landonly[loc][5][0] for loc in range(len(GNM_landonly))],c=[GNM_landonly[loc][0][day] for loc in range(len(GNM_landonly))],cmap='inferno_r',s=27,marker='s')\n",
    "    plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has to be done with the train set, so remember to change the JGNM experiment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting observations to us + Sample covariance matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GNM_landonly[location][param][day]\n",
    "# 0'pred_mu', 1'pred_disp', 2'target_did_rain', 3'target_rain_value', 4'pred_p', 5'location'\n",
    "def get_cdf_gnm(places):\n",
    "    out=[]\n",
    "    for idx,place in enumerate(places):\n",
    "        out.append([])    \n",
    "        for day in range(len(place[3])):\n",
    "            scaling = np.random.uniform(size=1)\n",
    "            obs = place[3][day]\n",
    "            p = place[4][day]\n",
    "\n",
    "            if obs==0: # dry day -> 1-p\n",
    "                out[idx].append(scaling*(1-p))\n",
    "            else: # rainy day -> (1-p)+p.cdf_x\n",
    "                mu,disp = place[0][day],place[1][day]\n",
    "                out[idx].append( (1-p)+(p*scs.gamma(scale=disp*mu,a=1/disp).cdf(obs)) )\n",
    "    return out\n",
    "\n",
    "obs_u_all_ = []\n",
    "for locs in tqdm(np.array_split(np.array(range(len(GNM_landonly))),67)):\n",
    "    \n",
    "    size = math.ceil(len(locs)/psutil.cpu_count())\n",
    "    locs_chuncks = [[GNM_landonly[i] for i in locs][x:x+size] for x in range(0,len(locs), size)]\n",
    "    \n",
    "    obs_u_all_.append( Parallel(n_jobs=psutil.cpu_count())(delayed(get_cdf_gnm)(chunck) for chunck in locs_chuncks) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "with open('obs_u_all_.txt','wb') as f:\n",
    "    pickle.dump(obs_u_all_,f)\n",
    "'''\n",
    "#Load \n",
    "with open('obs_u_all_.txt','rb') as f:\n",
    "    obs_u_all_ = pickle.load(f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "obs_u_all = np.concatenate(np.array(obs_u_all_).flatten()).flatten()\n",
    "obs_u_all = np.array(obs_u_all).astype(float)\n",
    "plt.scatter( range(len(obs_u_all)),obs_u_all,s=0.000005)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between adding and multiplying kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process.kernels import Matern\n",
    "M_topo,M_dist = Matern(length_scale=450,nu=3.5).__call__(topo_dist) , Matern(length_scale=450,nu=3.5).__call__(dist_mat) \n",
    "test_multiply =np.multiply( M_topo,M_dist)\n",
    "test_add =np.add((0.1/70) *M_topo,0.9*M_dist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(M_topo)\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.matshow(M_dist)\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.matshow(test_add)\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "plt.colorbar()\n",
    "\n",
    "cov_rain = np.cov(np.transpose(obs_norm_all))\n",
    "plt.matshow(cov_rain)\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.matshow(np.multiply(topo_dist[:2000,:2000],dist_mat[:2000,:2000]),cmap='viridis_r',vmax=13000)\n",
    "plt.colorbar()\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "plt.matshow(topo_dist[:2000,:2000],cmap='viridis_r')\n",
    "plt.colorbar()\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "plt.matshow(dist_mat[:2000,:2000],cmap='viridis_r')\n",
    "plt.colorbar()\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a local set of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the indexes (gives a nested array).Add [0] to get it as an array.\n",
    "np.where([math.dist(GNM_landonly[k][5],GNM_landonly[10][5])<2 for k in range(len(GNM_landonly))])[0]\n",
    "# gives the loctions themselves\n",
    "[GNM_landonly[i][5] for i in np.where([math.dist(GNM_landonly[k][5],GNM_landonly[10][5])<2 for k in range(len(GNM_landonly))])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacially subsampling\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "random_center = 3531\n",
    "locs = np.random.choice(np.where([math.dist(GNM_landonly[k][5],GNM_landonly[random_center ][5])<15 for k in range(len(GNM_landonly))])[0],400,replace=False)\n",
    "print(len(locs))\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(GNM_landonly))],\n",
    "[-GNM_landonly[loc][5][0] for loc in range(len(GNM_landonly))],\n",
    "color='grey',s=27,marker='s')\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in locs],\n",
    "[-GNM_landonly[loc][5][0] for loc in locs],\n",
    "c=[GNM_landonly[loc][3][day] for loc in locs],\n",
    "cmap='inferno_r',s=27,marker='s')\n",
    "\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsample_locs(locs_nb):\n",
    "    locs=[]\n",
    "    while len(locs)<locs_nb:\n",
    "        #random_center = np.random.choice(range(len(GNM_landonly))) \n",
    "        random_center = 3532  \n",
    "        locs = np.where([math.dist(GNM_landonly[k][5],GNM_landonly[random_center ][5])<(np.sqrt(locs_nb/np.pi)+3) for k in range(len(GNM_landonly))])[0]\n",
    "    return np.random.choice(locs,size=locs_nb,replace=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: make sure to get the same number of locs.\n",
    "\n",
    "# spacially subsampling\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "locs = generate_subsample_locs(locs_nb=400)\n",
    "print(len(locs))\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in locs],\n",
    "[-GNM_landonly[loc][5][0] for loc in locs],\n",
    "c=[GNM_landonly[loc][3][300] for loc in locs],\n",
    "cmap='inferno_r',s=27,marker='s')\n",
    "\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(np.array([GNM_landonly[l][5] for l in range(len(GNM_landonly))])): # closest to center 1107 [43 70]\n",
    "\n",
    "    if math.dist(v,[74,107])<2:\n",
    "        print(i,v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation with Scoring Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform observations into x with respect to their marginals and then to a normal scale. (1-p) + p*CDF(x), 1m30\n",
    "\n",
    "obs_norm_all = []\n",
    "for day in tqdm(range(len(GNM_landonly[0][1]))):\n",
    "    \n",
    "    # Transform observations into Us with respect to their marginals. (1-p) + p*CDF(x)\n",
    "    us = np.array(1-np.array([GNM_landonly[l][4][day] for l in range(len(GNM_landonly))]) + np.multiply( # 1-p +\n",
    "                        [GNM_landonly[l][4][day] for l in range(len(GNM_landonly))],                     # p *\n",
    "                        scs.gamma(                                                          # CDF(x)\n",
    "                            scale=np.multiply([GNM_landonly[l][0][day] for l in range(len(GNM_landonly))],[GNM_landonly[l][1][day] for l in range(len(GNM_landonly))]), # scale = mu * disp\n",
    "                            a=np.reciprocal([GNM_landonly[l][1][day] for l in range(len(GNM_landonly))])                                                   # a = shape = 1/disp \n",
    "                        ).cdf([GNM_landonly[l][3][day] for l in range(len(GNM_landonly))])               # x : observed rain\n",
    "                    ))\n",
    "\n",
    "    # Make sure no Us are above the precision of norm.ppf. Fix them to the next highest value of that day.\n",
    "    if np.sum([us>=1-(1e-15)])>0:\n",
    "        us[us>1]=np.max(us[us<1-(1e-15)])\n",
    "\n",
    "    # Transform Us into normal variables.\n",
    "    obs_norm_all.append(\n",
    "        scs.norm.ppf(us)\n",
    "    )\n",
    "obs_norm_all = np.nan_to_num(obs_norm_all, copy=True, nan=8.5, posinf=8.5, neginf=-8.5)\n",
    "\n",
    "censor_levels_di = []\n",
    "\n",
    "for day in tqdm(range(len(GNM_landonly[0][1]))):\n",
    "    \n",
    "    # Transform observations into Us with respect to their marginals. (1-p) + p*CDF(x)\n",
    "    ui = np.array(1-np.array([GNM_landonly[l][4][day] for l in range(len(GNM_landonly))])) # 1-p\n",
    "\n",
    "    #print(ui)\n",
    "\n",
    "    # Transform Us into normal variables.\n",
    "    censor_levels_di.append(\n",
    "        scs.norm.ppf(ui)\n",
    "    )\n",
    "\n",
    "censor_levels_di = np.array(censor_levels_di)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsample_locs(location_number):\n",
    "    l=[]\n",
    "    while len(l)<location_number:\n",
    "        random_center = np.random.choice(range(len(GNM_landonly))) \n",
    "        l = np.where([math.dist(GNM_landonly[k][5],GNM_landonly[random_center ][5])<(np.sqrt(location_number/np.pi)+30) for k in range(len(GNM_landonly))])[0]\n",
    "    return np.random.choice(l,size=location_number,replace=False)    \n",
    "\n",
    "subset_locations = []\n",
    "for s in range(250):\n",
    "    subset_locations.append(generate_subsample_locs(location_number=800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SR estimates\n",
    "\n",
    "def Spatial_Energy_Score_add_first(theta,locs_nb,days_nb,sim_nb,observations_x,censor_levels_di,dist_matrix,topo_matrix=None):\n",
    "    '''\n",
    "    theta: vector of 3 [Matern_param, coef_distmat, coef_topomat]. The coefs are applied to the matricies to form the ditance matrix used as input in the Matern kernel to form the covariance.\n",
    "    locs_subsets: subsets of locations with the same number of locs in each.\n",
    "    days_nb: how many days to include on the subsample.\n",
    "    sim_nb: number of simulation draws to compare with original data in the score.\n",
    "    observations_x: x, observed rainfall transformed to a Gaussian normal scale. This is the data that we try to emulate.\n",
    "    censor_levels_di: d, probability of no rain (1-p) transformed to a normal scale. These are used to censor simulations (x') when comparingt to x.\n",
    "    '''\n",
    "    # Subsample locations. Choose a random snippet from the subsets in locs_subsets.\n",
    "        # OLD: Subsample locations. Choose a random center and take all locations within fixed radius.\n",
    "    '''random_center = np.random.choice(range(len(GNM_landonly))) \n",
    "    locs = np.where([math.dist(GNM_landonly[k][5],GNM_landonly[random_center][5])<radius for k in range(len(GNM_landonly))])[0]\n",
    "    locs_nb = len(locs)'''\n",
    "        # New: pick one of the lists of location indexes from the locs_subsets provided.\n",
    "    locs = generate_subsample_locs(locs_nb)\n",
    "\n",
    "    # Subsample days.\n",
    "    days = np.random.choice(range(len(GNM_landonly[0][0])),size=days_nb,replace=False)\n",
    "\n",
    "    # Select relevant subsamples of data.\n",
    "    x = observations_x[days,:][:,locs]\n",
    "    d = censor_levels_di[days,:][:,locs]\n",
    "    \n",
    "    # Create covariance matrix.\n",
    "    if np.sum(topo_matrix==None):\n",
    "        cov_mat = np.nan_to_num(Matern(length_scale=theta[0],nu=3.5).__call__(dist_matrix[locs,:][:,locs]))\n",
    "    else:\n",
    "        cov_mat = np.nan_to_num(Matern(length_scale=theta[0],nu=3.5).__call__(theta[1]*dist_matrix[locs,:][:,locs]+topo_matrix[locs,:][:,locs]))\n",
    "    \n",
    "    # Draw samples from the multivariate nromal with appropriate covariance.\n",
    "    simulations = scs.multivariate_normal(mean=np.zeros(locs_nb),cov=cov_mat,allow_singular=True).rvs(size=sim_nb)\n",
    "    \n",
    "    # Censor simulated values and format them into [[sim0-day0,sim0-day1,...,sim0-dayn] ,..., [sim_m-day0,sim_m-day1,...,sim_m-dayn]]. Gives sim_nb*days_nb=m*n vectors of len=locs_nb. \n",
    "    censored_sims = np.maximum(np.repeat(simulations,days_nb,axis=0),np.tile(d,(sim_nb,1)))\n",
    "\n",
    "    # ||x' - x' ||_2 component of energy score\n",
    "    sims_by_day = [censored_sims[0+k::days_nb] for k in range(days_nb)] # regroup to get [[sim(1,day1), sim(2,day1),...,sim(m,day1)],...,[sim(1,dayn), sim(2,dayn),...,sim(m,dayn)]].\n",
    "    \n",
    "    sim_L2_diff = 0\n",
    "    \n",
    "    for sim_day_i in sims_by_day: # don't know how to make it more efficient... maybe can parallelize to compute complete score\n",
    "        \n",
    "        # ||x' - x' ||_2 component of energy score\n",
    "            # OLD: einsum from Lorenzos code on each sim_day_i was 9x slower then doing pdist on each sim_day_i. Both give the same answer\n",
    "        '''\n",
    "        # matrix of all combinations over i of [vector_i-vector_1, ... , vector_i-vector_n]\n",
    "        temp_diff_combinations_matrix = sim_day_i.reshape(1,sim_nb,locs_nb)-sim_day_i.reshape(sim_nb,1,locs_nb) \n",
    "        # einsum creates a matrix of sum of squares, then take sqrt to get L2.\n",
    "        sim_L2_diff+=0.5*np.sum(np.sqrt(np.einsum('ijk, ijk -> ij',temp_diff_combinations_matrix,temp_diff_combinations_matrix))) \n",
    "        '''\n",
    "            # New:\n",
    "        sim_L2_diff+=np.sum(pdist(sim_day_i))\n",
    "\n",
    "    # Add both parts of Energy.\n",
    "    Energy = np.mean(np.linalg.norm(censored_sims - np.tile(x,(sim_nb,1)),axis=1)) - sim_L2_diff/(days_nb*sim_nb*(sim_nb-1))\n",
    "    \n",
    "    out =2* Energy \n",
    "    path_.append([theta,out])\n",
    "    print([theta,out])\n",
    "    #print(theta,out)\n",
    "    return out\n",
    "\n",
    "def Spatial_Energy_Score_sublocs(theta,beta,sub_locs,days_nb,sim_nb,observations_x,censor_levels_di,dist_matrix,topo_matrix=None):\n",
    "    '''\n",
    "    theta: vector of 3 [Matern_param, coef_distmat, coef_topomat]. The coefs are applied to the matricies to form the ditance matrix used as input in the Matern kernel to form the covariance.\n",
    "    sub_locs: a set of subsets of location indexes with the same number of locs in each.\n",
    "    days_nb: how many days to include on the subsample.\n",
    "    sim_nb: number of simulation draws to compare with original data in the score.\n",
    "    observations_x: x, observed rainfall transformed to a Gaussian normal scale. This is the data that we try to emulate.\n",
    "    censor_levels_di: d, probability of no rain (1-p) transformed to a normal scale. These are used to censor simulations (x') when comparingt to x.\n",
    "    '''\n",
    "    # Subsample locations. Choose a random subset the list of subsets - sub_locs.\n",
    "    locs = sub_locs[np.random.choice(range(len(sub_locs)))] \n",
    "    locs_nb = len(locs)\n",
    "    # Subsample days.\n",
    "    days = np.random.choice(range(len(observations_x)),size=days_nb,replace=False)\n",
    "\n",
    "    # Select relevant subsamples of data.\n",
    "    x = observations_x[days,:][:,locs]\n",
    "    d = censor_levels_di[days,:][:,locs]\n",
    "    \n",
    "    # Create covariance matrix.\n",
    "    if np.sum(topo_matrix==None):\n",
    "        cov_mat = np.nan_to_num(Matern(length_scale=theta,nu=3.5).__call__(dist_matrix[locs,:][:,locs]))\n",
    "    else:\n",
    "        cov_mat = np.nan_to_num(Matern(length_scale=theta[0],nu=3.5).__call__(theta[1]*dist_mat[locs,:][:,locs]+((1-theta[1])/70)*topo_dist[locs,:][:,locs]))\n",
    "\n",
    "    # Draw samples from the multivariate nromal with appropriate covariance.\n",
    "    simulations = scs.multivariate_normal(mean=np.zeros(locs_nb),cov=cov_mat,allow_singular=True).rvs(size=sim_nb)\n",
    "    \n",
    "    # Censor simulated values and format them into [[sim0-day0,sim0-day1,...,sim0-dayn] ,..., [sim_m-day0,sim_m-day1,...,sim_m-dayn]]. Gives sim_nb*days_nb=m*n vectors of len=locs_nb. \n",
    "    censored_sims = np.maximum(np.repeat(simulations,days_nb,axis=0),np.tile(d,(sim_nb,1)))\n",
    "\n",
    "    # ||x' - x' ||_2 component of energy score\n",
    "    sims_by_day = [censored_sims[0+k::days_nb] for k in range(days_nb)] # regroup to get [[sim(1,day1), sim(2,day1),...,sim(m,day1)],...,[sim(1,dayn), sim(2,dayn),...,sim(m,dayn)]].\n",
    "    \n",
    "    sim_L2_diff = 0\n",
    "    \n",
    "    for sim_day_i in sims_by_day: # don't know how to make it more efficient... maybe can parallelize to compute complete score\n",
    "        \n",
    "        # ||x' - x' ||_2 component of energy score\n",
    "            # OLD: einsum from Lorenzos code on each sim_day_i was 9x slower then doing pdist on each sim_day_i. Both give the same answer\n",
    "        '''\n",
    "        # matrix of all combinations over i of [vector_i-vector_1, ... , vector_i-vector_n]\n",
    "        temp_diff_combinations_matrix = sim_day_i.reshape(1,sim_nb,locs_nb)-sim_day_i.reshape(sim_nb,1,locs_nb) \n",
    "        # einsum creates a matrix of sum of squares, then take sqrt to get L2.\n",
    "        sim_L2_diff+=0.5*np.sum(np.sqrt(np.einsum('ijk, ijk -> ij',temp_diff_combinations_matrix,temp_diff_combinations_matrix))) \n",
    "        '''\n",
    "            # New: pdist takes the offdiagonal of an L2 distance matrix, only counting once each combination of Y,Y'.\n",
    "        sim_L2_diff+= 2 * np.sum(np.power(pdist(sim_day_i),beta))\n",
    "\n",
    "    # Add both parts of Energy.\n",
    "    Energy = 2 * np.mean(np.power(np.linalg.norm(censored_sims - np.tile(x,(sim_nb,1)),axis=1),beta)) - sim_L2_diff/(days_nb*sim_nb*(sim_nb-1))\n",
    "    \n",
    "    \n",
    "    path_.append([theta,Energy])\n",
    "    print([theta,Energy])\n",
    "\n",
    "    return Energy\n",
    "\n",
    "path_avg = []\n",
    "\n",
    "def Spatial_Energy_Score_sublocs_averaged(theta,beta,sub_locs,days_nb,sim_nb,observations_x,censor_levels_di,dist_matrix,topo_matrix,avg_nb):\n",
    "    avg_SR = 0\n",
    "\n",
    "    for s in range(avg_nb):\n",
    "        avg_SR+= Spatial_Energy_Score_sublocs(theta,beta,sub_locs,days_nb,sim_nb,observations_x,censor_levels_di,dist_matrix,topo_matrix)\n",
    "\n",
    "    path_avg.append(avg_SR/avg_nb)\n",
    "    \n",
    "    return avg_SR/avg_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_=[]\n",
    "path_avg = []\n",
    "Spatial_Energy_Score_sublocs_averaged(theta=[800,0.9],\n",
    "                                      beta=0.2, \n",
    "                                      sub_locs=subset_locations, \n",
    "                                      days_nb=700,sim_nb=150, \n",
    "                                      observations_x=obs_norm_all, \n",
    "                                      censor_levels_di=censor_levels_di, \n",
    "                                      dist_matrix=dist_mat,\n",
    "                                      topo_matrix=topo_dist,\n",
    "                                      avg_nb=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiser\n",
    "\n",
    "path_ = []\n",
    "path_avg = [] # 100 sets of 800 locs, 700 days, 150 sims, beta=0.2 , avg of 4 =>25s/SRavg\n",
    "\n",
    "# In the case of the 2y robustness, 700 is all days in dataset, as len(GNM_landonly[0][0])=700 => only variation should come from subsampling locs and the sims themselves.\n",
    "dual_annealing(Spatial_Energy_Score_sublocs_averaged,[(100,1500),(0.5,0.99)],args=(0.2,subset_locations,700,150,obs_norm_all,censor_levels_di,dist_mat,topo_dist,4),no_local_search=True,x0=[310,0.9],maxiter=13000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Save\n",
    "with open('path_.txt','wb') as f:\n",
    "    pickle.dump(path_,f)\n",
    "'''\n",
    "#Load \n",
    "with open('path_.txt','rb') as f:\n",
    "    path_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.gcf()\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(range(len(path_)),path_[:,1],s=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(path_[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf() \n",
    "fig.set_size_inches(10,10)\n",
    "plt.hist(path_[:,1],bins=50)\n",
    "plt.title('SR')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(10,10)\n",
    "plt.tight_layout()\n",
    "plt.hist(path_[:,0],bins=50)\n",
    "plt.title('l_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf() \n",
    "fig.set_size_inches(10,10)\n",
    "plt.scatter(path_[:,0],path_[:,1],s=1)\n",
    "plt.title('l_scale vs SR')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN averaging for SR estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
    "knn.fit(path_[:,0].reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf() \n",
    "fig.set_size_inches(10,10)\n",
    "plt.tight_layout()\n",
    "for l in np.linspace(350,800,200):\n",
    "    plt.scatter(l,    np.mean([path_[c,1] for c in knn.kneighbors(np.array([[l]]),return_distance=False,n_neighbors=400)]),s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(15,10)\n",
    "l_scales = [np.mean(\n",
    "    [path_[c,1] for c in knn.kneighbors(np.array([[l]]),return_distance=False,n_neighbors=300)])\n",
    "     for l in np.linspace(200,1000,500)]\n",
    "\n",
    "plt.scatter( np.linspace(200,1000,500) , l_scales ,color='orange',s=30,label='Energy Score Estimates')\n",
    "plt.plot([450,450],[1.185,1.22],color='black',linestyle='dotted',linewidth=3,label=r'True $\\theta ^{*}$ = 450')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Energy Score')\n",
    "plt.savefig('SR_lscale_fakedata.eps', format='eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_scales = np.linspace(200,1500,50)\n",
    "a_coefs = np.linspace(0.5,1,50)\n",
    "L,A = np.meshgrid(l_scales,a_coefs)\n",
    "Z=np.zeros(L.shape)\n",
    "Z_variance=np.zeros(L.shape)\n",
    "for l in tqdm(range(Z.shape[0])):\n",
    "    for a in range(Z.shape[1]):\n",
    "        Z[l,a] = np.mean([np.array([path_[k][1] for k in range(len(path_))])[k] for k in knn.kneighbors(np.array([[L[l,a],A[l,a]]]),return_distance=False,n_neighbors=300)])\n",
    "        Z_variance[l,a] = np.std([np.array([path_[k][1] for k in range(len(path_))])[k] for k in knn.kneighbors(np.array([[L[l,a],A[l,a]]]),return_distance=False,n_neighbors=300)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf() \n",
    "fig.set_size_inches(10,10)\n",
    "plt.contourf(L,A,(Z),250,cmap='turbo')\n",
    "plt.colorbar()\n",
    "plt.scatter(450,0.9,s=200,c='red',label='True theta: [450,0.9]')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(10,10)\n",
    "plt.contourf(L,A,Z_variance,250,cmap='turbo')\n",
    "plt.colorbar()\n",
    "plt.scatter(450,0.9,s=200,c='red',label='True theta: [450,0.9]')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf() \n",
    "fig.set_size_inches(15,10)\n",
    "plt.tricontourf([path_[k][0][0] for k in range(len(path_))], [path_[k][0][1] for k in range(len(path_))], [path_[k][1] for k in range(len(path_))], levels=100, linewidths=0.5,cmap='viridis_r')\n",
    "plt.colorbar()\n",
    "plt.scatter([path_[k][0][0] for k in range(len(path_))], [path_[k][0][1] for k in range(len(path_))], c=[path_[k][1] for k in range(len(path_))],s=0.5,cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.title('simulated cov_mat with GNM data. true theta=(10000,1.7). 100 fixed locs,200 foxed days, Energy SR, evaluated over all data at each iter. Dual anni after 1000feval')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_over6_5_SR = np.argwhere(np.array([path_[k][1] for k in range(len(path_))])<1.5).flatten()\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(15,10)\n",
    "plt.tricontourf([path_[k][0][0] for k in idx_over6_5_SR], [path_[k][0][1] for k in idx_over6_5_SR], [path_[k][1] for k in idx_over6_5_SR], levels=100, linewidths=0.5,cmap='viridis_r')\n",
    "plt.colorbar()\n",
    "plt.scatter([path_[k][0][0] for k in idx_over6_5_SR], [path_[k][0][1] for k in idx_over6_5_SR], c=[path_[k][1] for k in idx_over6_5_SR],s=11,cmap='Oranges')\n",
    "plt.colorbar()\n",
    "plt.title('simulated cov_mat with GNM data. true theta=(10000,1.7). 100 fixed locs,200 foxed days, Energy SR, evaluated over all data at each iter. Dual anni after 1000feval. Only showing points where SR<6.45 =>99 points')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments for SR estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_locations = []\n",
    "def generate_subsample_locs(locs_nb):\n",
    "    locs=[]\n",
    "    while len(locs)<locs_nb:\n",
    "        random_center = np.random.choice(range(len(GNM_landonly))) \n",
    "        locs = np.where([math.dist(GNM_landonly[k][5],GNM_landonly[random_center ][5])<(np.sqrt(locs_nb/np.pi)+3) for k in range(len(GNM_landonly))])[0]\n",
    "    return np.random.choice(locs,size=locs_nb,replace=False)    \n",
    "for s in range(50):\n",
    "    subset_locations.append(generate_subsample_locs(locs_nb=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spatial_Energy_Score_exp(theta,beta,sub_locs,days_nb,sim_nb,observations_x,censor_levels_di,dist_matrix,topo_matrix=None):\n",
    "    '''\n",
    "    theta: vector of 3 [Matern_param, coef_distmat, coef_topomat]. The coefs are applied to the matricies to form the ditance matrix used as input in the Matern kernel to form the covariance.\n",
    "    sub_locs: a set of subsets of location indexes with the same number of locs in each.\n",
    "    days_nb: how many days to include on the subsample.\n",
    "    sim_nb: number of simulation draws to compare with original data in the score.\n",
    "    observations_x: x, observed rainfall transformed to a Gaussian normal scale. This is the data that we try to emulate.\n",
    "    censor_levels_di: d, probability of no rain (1-p) transformed to a normal scale. These are used to censor simulations (x') when comparingt to x.\n",
    "    '''\n",
    "    # Subsample locations. Choose a random subset the list of subsets - sub_locs.\n",
    "    locs = sub_locs \n",
    "    locs_nb = len(locs)\n",
    "    # Subsample days.\n",
    "    days = np.random.choice(range(len(observations_x)),size=days_nb,replace=False)\n",
    "\n",
    "    # Select relevant subsamples of data.\n",
    "    x = observations_x[days,:]\n",
    "    d = censor_levels_di[days,:]\n",
    "    \n",
    "    # Create covariance matrix.\n",
    "    if np.sum(topo_matrix==None):\n",
    "        cov_mat = np.nan_to_num(Matern(length_scale=theta,nu=3.5).__call__(dist_matrix[locs,:][:,locs]))\n",
    "    else:\n",
    "        cov_mat = np.nan_to_num(Matern(length_scale=theta,nu=3.5).__call__(0.9*dist_mat[locs,:][:,locs]+((0.1)/70)*topo_dist[locs,:][:,locs]))\n",
    "\n",
    "    # Draw samples from the multivariate nromal with appropriate covariance.\n",
    "    simulations = scs.multivariate_normal(mean=np.zeros(locs_nb),cov=cov_mat,allow_singular=True).rvs(size=sim_nb)\n",
    "    \n",
    "    # Censor simulated values and format them into [[sim0-day0,sim0-day1,...,sim0-dayn] ,..., [sim_m-day0,sim_m-day1,...,sim_m-dayn]]. Gives sim_nb*days_nb=m*n vectors of len=locs_nb. \n",
    "    censored_sims = np.maximum(np.repeat(simulations,days_nb,axis=0),np.tile(d,(sim_nb,1)))\n",
    "\n",
    "    # ||x' - x' ||_2 component of energy score\n",
    "    sims_by_day = [censored_sims[0+k::days_nb] for k in range(days_nb)] # regroup to get [[sim(1,day1), sim(2,day1),...,sim(m,day1)],...,[sim(1,dayn), sim(2,dayn),...,sim(m,dayn)]].\n",
    "    \n",
    "    sim_L2_diff = 0\n",
    "    \n",
    "    for sim_day_i in sims_by_day: # don't know how to make it more efficient... maybe can parallelize to compute complete score\n",
    "        \n",
    "        # ||x' - x' ||_2 component of energy score\n",
    "            # OLD: einsum from Lorenzos code on each sim_day_i was 9x slower then doing pdist on each sim_day_i. Both give the same answer\n",
    "        '''\n",
    "        # matrix of all combinations over i of [vector_i-vector_1, ... , vector_i-vector_n]\n",
    "        temp_diff_combinations_matrix = sim_day_i.reshape(1,sim_nb,locs_nb)-sim_day_i.reshape(sim_nb,1,locs_nb) \n",
    "        # einsum creates a matrix of sum of squares, then take sqrt to get L2.\n",
    "        sim_L2_diff+=0.5*np.sum(np.sqrt(np.einsum('ijk, ijk -> ij',temp_diff_combinations_matrix,temp_diff_combinations_matrix))) \n",
    "        '''\n",
    "            # New: pdist takes the offdiagonal of an L2 distance matrix, only counting once each combination of Y,Y'.\n",
    "        sim_L2_diff+= 2 * np.sum(np.power(pdist(sim_day_i),beta))\n",
    "\n",
    "    # Add both parts of Energy.\n",
    "    Energy = 2 * np.mean(np.power(np.linalg.norm(censored_sims - np.tile(x,(sim_nb,1)),axis=1),beta)) - sim_L2_diff/(days_nb*sim_nb*(sim_nb-1))\n",
    "    \n",
    "    \n",
    "    path_.append([theta,Energy])\n",
    "    print([theta,Energy])\n",
    "\n",
    "    return Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating fake data\n",
    "\n",
    "# 1 - fixing a theta and generating data from that, then estimating to retrieve the same theta.\n",
    "def generate_subsample_locs(location_number):\n",
    "    l=[]\n",
    "    while len(l)<location_number:\n",
    "        random_center = np.random.choice(range(len(GNM_landonly))) \n",
    "        l = np.where([math.dist(GNM_landonly[k][5],GNM_landonly[random_center ][5])<(np.sqrt(location_number/np.pi)+2) for k in range(len(GNM_landonly))])[0]\n",
    "    return np.random.choice(l,size=location_number,replace=False)   \n",
    " \n",
    "'''\n",
    "subset_locations = []\n",
    "for s in range(250):\n",
    "    subset_locations.append(generate_subsample_locs(location_number=800))\n",
    "'''\n",
    "fake_locs=generate_subsample_locs(400)\n",
    "fake_theta = [450,0.9]\n",
    "fake_cov_mat =np.nan_to_num(Matern(length_scale=fake_theta[0],nu=3.5).__call__(fake_theta[1]*dist_mat[fake_locs,:][:,fake_locs]+((1-fake_theta[1])/70)*topo_dist[fake_locs,:][:,fake_locs])) \n",
    "fake_data = scs.multivariate_normal(mean=np.zeros(len(fake_locs)),cov=fake_cov_mat,allow_singular=True).rvs(size=5000)\n",
    "for day,day_data in enumerate(fake_data):\n",
    "    fake_data[day]=np.maximum(day_data,[censor_levels_di[day][l] for l in fake_locs])\n",
    "fake_censor_levels_di = censor_levels_di[range(5000),:][:,fake_locs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.9*dist_mat[fake_locs,:][:,fake_locs]+((1-0.9)/70)*topo_dist[fake_locs,:][:,fake_locs]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(GNM_landonly))],[-GNM_landonly[loc][5][0] for loc in range(len(GNM_landonly))],c='grey',alpha=0.3,s=27,marker='s')\n",
    "plt.scatter(GNM_landonly[locs[0]][5][1],\n",
    "-GNM_landonly[locs[0]][5][0],\n",
    "c='orange',label='Subset of locations',\n",
    "cmap='inferno_r',s=100,marker='s')\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in locs],\n",
    "[-GNM_landonly[loc][5][0] for loc in locs],\n",
    "c=[GNM_landonly[loc][3][300] for loc in locs],\n",
    "cmap='inferno_r',s=21,marker='s')\n",
    "plt.legend()\n",
    "plt.yticks(ticks=np.linspace(-100,0,5),labels=np.linspace(0,100,5))\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.savefig('sub_locs.eps', format='eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(GNM_landonly))],[-GNM_landonly[loc][5][0] for loc in range(len(GNM_landonly))],c='grey',alpha=0.3,s=27,marker='s')\n",
    "plt.scatter(GNM_landonly[fake_locs[0]][5][1],\n",
    "-GNM_landonly[fake_locs[0]][5][0],\n",
    "c='orange',label='Subset of locations',\n",
    "cmap='inferno_r',s=100,marker='s')\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in fake_locs],\n",
    "[-GNM_landonly[loc][5][0] for loc in fake_locs],\n",
    "c=[GNM_landonly[loc][3][300] for loc in fake_locs],\n",
    "cmap='inferno_r',s=21,marker='s')\n",
    "plt.legend()\n",
    "plt.yticks(ticks=np.linspace(-100,0,5),labels=np.linspace(0,100,5))\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100 sets of 800 locs, 700 days, 150 sims, beta=0.2 , avg of 4 =>25s/SRavg\n",
    "path_=[]\n",
    "Spatial_Energy_Score_exp(theta=450,\n",
    "                                      beta=0.2, \n",
    "                                      sub_locs=fake_locs, \n",
    "                                      days_nb=700,sim_nb=150, \n",
    "                                      observations_x=fake_data, \n",
    "                                      censor_levels_di=fake_censor_levels_di, \n",
    "                                      dist_matrix=dist_mat,\n",
    "                                      topo_matrix=topo_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq=[[],[]]\n",
    "for a in tqdm(range(20)):\n",
    "    qqq[1].append(Spatial_Energy_Score_sublocs(theta=[30000,3],beta=0.2,sub_locs=subset_locations,days_nb=700,sim_nb=100,observations_x=fake_data,censor_levels_di=censor_levels_di,dist_matrix=dist_mat,topo_matrix=topo_dist)\n",
    ")\n",
    "    qqq[0].append(Spatial_Energy_Score_sublocs(theta=[100,4],beta=0.2,sub_locs=subset_locations,days_nb=700,sim_nb=100,observations_x=fake_data,censor_levels_di=censor_levels_di,dist_matrix=dist_mat,topo_matrix=topo_dist)\n",
    ")\n",
    "plt.plot(qqq[0])\n",
    "plt.plot(qqq[1],color='red')\n",
    "np.mean(np.array(qqq),axis=1) # 750 locations 700 days 100 sims, beta = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_ = []\n",
    "\n",
    "dual_annealing(Spatial_Energy_Score_exp,[(100,1000)],args=(0.2,fake_locs,700,150,fake_data,fake_censor_levels_di,dist_mat,topo_dist),no_local_search=True,maxiter=13000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with simple Gaussian spatial copula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_locs=locs # got these from 'finding a local set of points' section\n",
    "fake_theta = [450,0.9]\n",
    "fake_cov_mat =np.nan_to_num(Matern(length_scale=fake_theta[0],nu=3.5).__call__(fake_theta[1]*dist_mat[fake_locs,:][:,fake_locs]+((1-fake_theta[1])/70)*topo_dist[fake_locs,:][:,fake_locs])) \n",
    "fake_data = scs.multivariate_normal(mean=np.zeros(len(fake_locs)),cov=fake_cov_mat,allow_singular=True).rvs(size=5000) #5000 samples\n",
    "\n",
    "def Spatial_Energy_Score_exp_classic(theta,beta,sub_locs,days_nb,sim_nb,observations_x,dist_matrix,topo_matrix=None):\n",
    "    '''\n",
    "    theta: vector of 3 [Matern_param, coef_distmat, coef_topomat]. The coefs are applied to the matricies to form the ditance matrix used as input in the Matern kernel to form the covariance.\n",
    "    sub_locs: a set of subsets of location indexes with the same number of locs in each.\n",
    "    days_nb: how many days to include on the subsample.\n",
    "    sim_nb: number of simulation draws to compare with original data in the score.\n",
    "    observations_x: x, observed rainfall transformed to a Gaussian normal scale. This is the data that we try to emulate.\n",
    "    censor_levels_di: d, probability of no rain (1-p) transformed to a normal scale. These are used to censor simulations (x') when comparingt to x.\n",
    "    '''\n",
    "    # Subsample locations. Choose a random subset the list of subsets - sub_locs.\n",
    "    locs = sub_locs \n",
    "    locs_nb = len(locs)\n",
    "    # Subsample days.\n",
    "    days = np.random.choice(range(len(observations_x)),size=days_nb,replace=False)\n",
    "\n",
    "    # Select relevant subsamples of data.\n",
    "    x = observations_x[days,:]\n",
    "    \n",
    "    # Create covariance matrix.\n",
    "    if np.sum(topo_matrix==None):\n",
    "        cov_mat = np.nan_to_num(Matern(length_scale=theta,nu=3.5).__call__(dist_matrix[locs,:][:,locs]))\n",
    "    else:\n",
    "        cov_mat = np.nan_to_num(Matern(length_scale=theta,nu=3.5).__call__(0.9*dist_mat[locs,:][:,locs]+((0.1)/70)*topo_dist[locs,:][:,locs]))\n",
    "\n",
    "    # Draw samples from the multivariate nromal with appropriate covariance.\n",
    "    simulations = scs.multivariate_normal(mean=np.zeros(locs_nb),cov=cov_mat,allow_singular=True).rvs(size=sim_nb)\n",
    "    \n",
    "    censored_sims = np.repeat(simulations,days_nb,axis=0)\n",
    "\n",
    "    # ||x' - x' ||_2 component of energy score\n",
    "    sims_by_day = [censored_sims[0+k::days_nb] for k in range(days_nb)] # regroup to get [[sim(1,day1), sim(2,day1),...,sim(m,day1)],...,[sim(1,dayn), sim(2,dayn),...,sim(m,dayn)]].\n",
    "    \n",
    "    sim_L2_diff = 0\n",
    "    \n",
    "    for sim_day_i in sims_by_day: # don't know how to make it more efficient... maybe can parallelize to compute complete score\n",
    "        \n",
    "        # ||x' - x' ||_2 component of energy score\n",
    "            # OLD: einsum from Lorenzos code on each sim_day_i was 9x slower then doing pdist on each sim_day_i. Both give the same answer\n",
    "        '''\n",
    "        # matrix of all combinations over i of [vector_i-vector_1, ... , vector_i-vector_n]\n",
    "        temp_diff_combinations_matrix = sim_day_i.reshape(1,sim_nb,locs_nb)-sim_day_i.reshape(sim_nb,1,locs_nb) \n",
    "        # einsum creates a matrix of sum of squares, then take sqrt to get L2.\n",
    "        sim_L2_diff+=0.5*np.sum(np.sqrt(np.einsum('ijk, ijk -> ij',temp_diff_combinations_matrix,temp_diff_combinations_matrix))) \n",
    "        '''\n",
    "            # New: pdist takes the offdiagonal of an L2 distance matrix, only counting once each combination of Y,Y'.\n",
    "        sim_L2_diff+= 2 * np.sum(np.power(pdist(sim_day_i),beta))\n",
    "    # Add both parts of Energy.\n",
    "    Energy = 2 * np.mean(np.power(np.linalg.norm(censored_sims - np.tile(x,(sim_nb,1)),axis=1),beta)) - sim_L2_diff/(days_nb*sim_nb*(sim_nb-1))\n",
    "    \n",
    "    \n",
    "    path_.append([theta,Energy])\n",
    "    print([theta,Energy])\n",
    "\n",
    "    return Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Spatial_Energy_Score_exp_classic(theta=200,beta=0.5,sub_locs=fake_locs,days_nb=3000,sim_nb=150,observations_x=fake_data,dist_matrix=dist_mat,topo_matrix=topo_dist) for k in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = []\n",
    "simple_gaussian_experiment = []\n",
    "for theta_val in tqdm(np.linspace(200,1000,500)):\n",
    "\n",
    "    avg = np.mean([Spatial_Energy_Score_exp_classic(theta=theta_val,beta=0.5,sub_locs=fake_locs,days_nb=3000,sim_nb=150,observations_x=fake_data,dist_matrix=dist_mat,topo_matrix=topo_dist) for i in range(15)])\n",
    "    simple_gaussian_experiment.append(avg)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_gaussian_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = np.array(path_)\n",
    "plt.scatter(path_[:,0],path_[:,1])\n",
    "plt.plot(np.linspace(200,1000,500),simple_gaussian_experiment,color='red')\n",
    "min_idx=np.where(simple_gaussian_experiment==np.min(simple_gaussian_experiment))[0]\n",
    "\n",
    "plt.plot([np.linspace(200,1000,500)[min_idx],np.linspace(200,1000,500)[min_idx]],[4.4,4.9],color='black',linestyle='dotted',linewidth=3,label=r'Estimated $\\theta ^{*}$ = '+str(np.linspace(200,1000,500)[min_idx]))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(30):\n",
    "    sr_avg_scores = [np.mean([simple_gaussian_experiment[i:i+k]]) for i in range(len(simple_gaussian_experiment))]\n",
    "    print(k,np.linspace(200, 1000, 500)[np.where(sr_avg_scores==np.min(sr_avg_scores))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 30})\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(15,10)\n",
    "\n",
    "\n",
    "#plt.scatter( np.linspace(200,1000,500) , simple_gaussian_experiment ,color='orange',s=30,label='Energy Score Estimates')\n",
    "plt.scatter(np.linspace(200,1000,495) , [np.mean([simple_gaussian_experiment[i:i+10]]) for i in range(len(simple_gaussian_experiment)-5)],color='orange' ,s=30,label=' Energy Score Estimates')\n",
    "plt.plot([450,450],[4.61,4.69],color='black',linestyle='dotted',linewidth=3,label=r'True $\\theta ^{*}$ = 450')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('Energy Score')\n",
    "plt.savefig('SR_lscale_fakedata_simple_gauss2.eps', format='eps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('path_ex_simple_gaussian_attempt2.txt','wb') as f:\n",
    "    pickle.dump([path_,simple_gaussian_experiment],f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cov_rain = np.cov(np.transpose(obs_norm_all))\n",
    "#sim_cov_rain = scs.multivariate_normal(mean=np.zeros(len(dist_mat)),cov=cov_rain).rvs(size=25)\n",
    "theta = [350,0.9] \n",
    "sim_test = scs.multivariate_normal(mean=np.zeros(len(dist_mat)),cov= np.nan_to_num(Matern(length_scale=theta[0],nu=3.5).__call__(theta[1]*dist_mat+((1-theta[1])/70)*topo_dist)) ,allow_singular=True).rvs(size=100)\n",
    "\n",
    "#theta2 = [310, 1] \n",
    "#sim_test2 = scs.multivariate_normal(mean=np.zeros(len(dist_mat)),cov= np.nan_to_num(Matern(length_scale=theta2[0],nu=3.5).__call__(theta[1]*dist_mat+((1-theta[1])/70)*topo_dist)) ,allow_singular=True).rvs(size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GNM_ppf(x_one_day_all_locs,day_idx):\n",
    "    m = np.array([GNM_landonly[l][0][day_idx] for l in range(len(x_one_day_all_locs))])\n",
    "    d = np.array([GNM_landonly[l][1][day_idx] for l in range(len(x_one_day_all_locs))])\n",
    "    p = np.array([GNM_landonly[l][4][day_idx] for l in range(len(x_one_day_all_locs))])\n",
    "    sim_u = scs.norm.cdf(x_one_day_all_locs)\n",
    "    censored_u = (sim_u+p-1)/p\n",
    "    return np.nan_to_num(scs.gamma(a=1/d,scale=m*d).ppf(censored_u),nan=0)\n",
    "    \n",
    "\n",
    "#GNM_landonly[location][param][day]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ampled normals_uncesored\n",
    "\n",
    "day_index = 320\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=[GNM_landonly[l][3][day_index] for l in range(len(GNM_landonly))][:len(sim_test[0])],cmap='inferno_r',s=27,marker='s')\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('Observed rain')\n",
    "plt.show()\n",
    "\n",
    "min_rain = min([GNM_landonly[l][3][day_index] for l in range(len(GNM_landonly))])\n",
    "max_rain = max([GNM_landonly[l][3][day_index] for l in range(len(GNM_landonly))])\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=np.median(np.array([GNM_ppf(sim,day_index) for sim in sim_test]),axis=0),cmap='inferno_r',s=27,marker='s',vmin=min_rain,vmax=max_rain)\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('Median simulated')\n",
    "plt.show()\n",
    "\n",
    "for idx,sim in enumerate(sim_test[:10]):\n",
    "    '''fig = plt.gcf() \n",
    "    fig.set_size_inches(14,10)\n",
    "\n",
    "    plt.xlim(0,140)\n",
    "    plt.ylim(-100,0)\n",
    "    plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=GNM_ppf(sim_test2[idx][:len(sim_test[0])],day_index),cmap='inferno_r',s=27,marker='s')\n",
    "    plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "    plt.title(' test 2')\n",
    "    plt.show()'''\n",
    "\n",
    "    plt.xlim(0,140)\n",
    "    plt.ylim(-100,0)\n",
    "    fig = plt.gcf() \n",
    "    fig.set_size_inches(14,10)\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=GNM_ppf(sim,day_index),cmap='inferno_r',s=27,marker='s',vmin=min_rain,vmax=max_rain)\n",
    "    plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "    plt.title('Simulated rain with distance only')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing biases introduces by copula sampling against GNM marginals.\n",
    "\n",
    "true_median = scs.gamma(a=np.array([1/GNM_landonly[l][1][140] for l in range(len(GNM_landonly))]),\n",
    "          scale=np.multiply( np.array([GNM_landonly[l][0][140] for l in range(len(GNM_landonly))]),np.array([GNM_landonly[l][1][140] for l in range(len(GNM_landonly))]) )).ppf(\n",
    "    np.divide((np.array([GNM_landonly[l][4][140] for l in range(len(GNM_landonly))])-0.5),np.array([GNM_landonly[l][4][140] for l in range(len(GNM_landonly))]))\n",
    "          )\n",
    "sim_median = np.mean(np.array([GNM_ppf(sim_test[s],140) for s in range(len(sim_test))]),axis=0)\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=[GNM_landonly[loc][3][140] for loc in range(len(sim_test[0]))],cmap='inferno_r',s=2.8,marker='s')\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('true rain')\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()\n",
    "min_rain = np.min([GNM_landonly[loc][3][140] for loc in range(len(sim_test[0]))])\n",
    "max_rain = np.max([GNM_landonly[loc][3][140] for loc in range(len(sim_test[0]))])\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=[GNM_landonly[loc][0][140] for loc in range(len(sim_test[0]))],cmap='inferno_r',s=2.8,marker='s',vmin=min_rain,vmax=max_rain)\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('GNM mean')\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=true_median,cmap='inferno_r',s=2.7,marker='s',vmin=min_rain,vmax=max_rain)\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('GNM_median')\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=sim_median,cmap='inferno_r',s=2.7,marker='s',vmin=min_rain,vmax=max_rain)\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('sim_median')\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "max_dif = np.max(np.abs(true_median-sim_median))\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=true_median-sim_median,cmap='RdBu',s=27,marker='s',vmin=-max_dif,vmax=max_dif)\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('GNM_median-sim_median')\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "max_dif = np.max(np.abs([GNM_landonly[loc][0][140] for loc in range(len(sim_test[0]))]-sim_median))\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=[GNM_landonly[loc][0][140] for loc in range(len(sim_test[0]))]-sim_median,cmap='RdBu',s=27,marker='s',vmin=-max_dif,vmax=max_dif)\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('GNM_mean-sim_median')\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(14,10)\n",
    "max_dif = np.max(np.abs([GNM_landonly[loc][3][140] for loc in range(len(sim_test[0]))]-sim_median))\n",
    "plt.scatter([GNM_landonly[loc][5][1] for loc in range(len(sim_test[0]))],[-GNM_landonly[loc][5][0] for loc in range(len(sim_test[0]))],c=[GNM_landonly[loc][3][140] for loc in range(len(sim_test[0]))]-sim_median,cmap='RdBu',s=27,marker='s',vmin=-max_dif,vmax=max_dif)\n",
    "plt.colorbar(location='bottom',fraction=0.0652,pad=0.001)\n",
    "plt.title('true_rain-sim_median')\n",
    "plt.xlim(0,140)\n",
    "plt.ylim(-100,0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrix testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_rain = np.cov(np.transpose(obs_norm_all))\n",
    "sim_cov_rain = scs.multivariate_normal(mean=np.zeros(len(dist_mat)),cov=cov_rain).rvs(size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(dist_mat)\n",
    "plt.colorbar()\n",
    "plt.matshow(topo_dist)#\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_mat = [0,4087]\n",
    "theta = [60,0.045,1/1500]\n",
    "# SR [1.53735034e+02,3.5, 5.99074130e+03,3.5 ] #MLE:[208.33370545627545,3.5, 4500.88496935951,3.5]\n",
    "\n",
    "plt.matshow(cov_rain[lim_mat[0]:lim_mat[1],lim_mat[0]:lim_mat[1]],cmap='viridis_r',vmin=0)\n",
    "plt.colorbar()\n",
    "plt.title('cov')\n",
    "\n",
    "\n",
    "topo_matern = np.nan_to_num(Matern(length_scale=12000,nu=3.5).__call__(topo_dist[lim_mat[0]:lim_mat[1],lim_mat[0]:lim_mat[1]]))\n",
    "dist_matern= np.nan_to_num(Matern(length_scale=600,nu=3.5).__call__(dist_mat[lim_mat[0]:lim_mat[1],lim_mat[0]:lim_mat[1]]))\n",
    "cov_matern_add = 0.8* np.nan_to_num(Matern(length_scale=theta[0],nu=3.5).__call__(theta[1]*dist_mat[lim_mat[0]:lim_mat[1],lim_mat[0]:lim_mat[1]]+theta[2]*topo_dist[lim_mat[0]:lim_mat[1],lim_mat[0]:lim_mat[1]]))\n",
    "plt.matshow(cov_matern_add,cmap='viridis_r',vmin=0,vmax=1.1)\n",
    "plt.colorbar()\n",
    "plt.title('Adding distances before Matern kernel')\n",
    "\n",
    "plt.matshow(dist_matern,cmap='viridis_r',vmin=0)\n",
    "plt.title('distance matern')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.matshow(topo_matern,cmap='viridis_r',vmin=0)\n",
    "plt.title('topo matern')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_mat = [0,2000]\n",
    "\n",
    "plt.matshow(Matern(length_scale=9000,nu=3.5).__call__(dist_mat[lim_mat[0]:lim_mat[1],lim_mat[0]:lim_mat[1]]),cmap='viridis_r',vmin=0)\n",
    "plt.colorbar()\n",
    "plt.title('cov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(cov_rain[lim_mat[0]:lim_mat[1],lim_mat[0]:lim_mat[1]],cmap='viridis_r',vmin=0)\n",
    "plt.colorbar()\n",
    "plt.title('cov')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "fn = 'topo_0.1_degree.nc'\n",
    "ds = nc.Dataset(fn)\n",
    "h = ds['Band1'][:]\n",
    "h = np.flip(np.matrix(h),axis=0)\n",
    "topo_dist = euclidean_distances(np.array([h[loc[5][0],loc[5][1]+2] for loc in GNM_landonly]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(topo_dist)\n",
    "plt.show()\n",
    "plt.matshow(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab60c10c504ddb966051b34fe3c8284e278512e8f8393189136f471cdd0fd1b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
